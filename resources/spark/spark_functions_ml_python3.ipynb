{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(last updated:   8/23/16, Reshama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "Spark is the future, and in many ways the present.  It lets us work with a lot of the concepts we've covered at scale, combining some of the best aspects of Hadoop with a smarter execution engine for problems that aren't really MapReduce. \n",
    "\n",
    "In this notebook we'll examine some of the primitives that Spark has for transforming data in a distributed fashion, as well as use MLLib to implement machine learning in Spark.\n",
    "\n",
    "## PySpark\n",
    "PySpark is the Python binding for Spark, so that's how we'll investigate Spark in this notebook.  To get it up and running, you'll have to go through some gymnastics like the next few cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "Spark Programming Guide:  \n",
    "http://spark.apache.org/docs/latest/programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "[Example 01 - word count](#1) \n",
    "\n",
    "[Example 02 - word count](#2)\n",
    "\n",
    "[Example 03 - word count (more complicated)](#3)\n",
    "\n",
    "[Example 04:  numerical (prime numbers)](#4)\n",
    "\n",
    "[Example 05:  logistic regression with Spark MLlib](#5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python 3 in this notebook (make sure kernel matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 :: Continuum Analytics, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this might give you an error\n",
    "#import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): findspark in /opt/conda/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "# install findspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finds the location of spark installation\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to see which packages, libraries are installed\n",
    "#!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A spark context is the main entry point for Spark functionality. It is the connection to the Spark cluster and can be used to creat RDDs, accumulators and broadcast variables on that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fire up a Spark context\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: we can only run the cell below once (re-running the SparkContext() command will give an error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f164c39b2e8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_home = os.environ.get('SPARK_HOME', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark\n"
     ]
    }
   ],
   "source": [
    "print (spark_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from collections import namedtuple\n",
    "from operator import add, itemgetter\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import csv\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='1'></a> Example 1:  Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have PySpark up and running, let's try out the canonical word count example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the Spark course file with a simple call to textFile\n",
    "text_file = sc.textFile(\"0_spark_about.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of lines in text file\n",
    "count = text_file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '# SPARK',\n",
       " '',\n",
       " \"Spark is the future, Spark is right now.  The goal is essentially an end-to-end platform for data science on Big Data, the way sklearn is for small data.  It's not there yet, but it's growing daily.\",\n",
       " '',\n",
       " '## WHAT IS Spark?',\n",
       " '',\n",
       " '\\t* Spark is a fast and general engine for large-scale distributed data processing.',\n",
       " '\\t* Spark can run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.',\n",
       " \"\\t* Spark improves on MapReduce's computation model with an advanced DAG (Directed Acyclic Graph) execution engine that supports cyclic data flow and in-memory computing.\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 10 lines of file\n",
    "text_file.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RDD = Resilient Distributed Dataset. This is an immutable, partitioned collection of elements that can be operated upon in parallel\n",
    "Let's do some word counting on this RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can apply a filter using an anonymous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines_not_empty = text_file.filter(lambda x: len(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines_not_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_not_empty.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can use the `take()` function to retrieve items from our RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# SPARK',\n",
       " \"Spark is the future, Spark is right now.  The goal is essentially an end-to-end platform for data science on Big Data, the way sklearn is for small data.  It's not there yet, but it's growing daily.\",\n",
       " '## WHAT IS Spark?',\n",
       " '\\t* Spark is a fast and general engine for large-scale distributed data processing.',\n",
       " '\\t* Spark can run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.',\n",
       " \"\\t* Spark improves on MapReduce's computation model with an advanced DAG (Directed Acyclic Graph) execution engine that supports cyclic data flow and in-memory computing.\",\n",
       " '\\t* Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.',\n",
       " '\\t* Spark is written in Scala, but has bindings for Java, Python (PySpark), and R',\n",
       " '\\t',\n",
       " '### Spark Modules']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_not_empty.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### flatMap() \n",
    "- flattens the return lists into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = text_file.flatMap(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words in whole file\n",
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'SPARK',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'the',\n",
       " 'future,',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'right',\n",
       " 'now.',\n",
       " 'The',\n",
       " 'goal',\n",
       " 'is',\n",
       " 'essentially',\n",
       " 'an',\n",
       " 'end-to-end',\n",
       " 'platform',\n",
       " 'for',\n",
       " 'data',\n",
       " 'science',\n",
       " 'on',\n",
       " 'Big',\n",
       " 'Data,',\n",
       " 'the',\n",
       " 'way',\n",
       " 'sklearn',\n",
       " 'is',\n",
       " 'for',\n",
       " 'small',\n",
       " 'data.',\n",
       " \"It's\",\n",
       " 'not',\n",
       " 'there',\n",
       " 'yet,',\n",
       " 'but',\n",
       " \"it's\",\n",
       " 'growing',\n",
       " 'daily.',\n",
       " '##',\n",
       " 'WHAT',\n",
       " 'IS',\n",
       " 'Spark?',\n",
       " '*',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fast',\n",
       " 'and',\n",
       " 'general',\n",
       " 'engine']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The map function\n",
    "- map returns a new RDD containing values created by applying the supplied lambda function to each value in the original RDD\n",
    "- A map function utilizing the anonymous Python function lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = words.map(lambda x: x.replace('|', '').replace('.', '').\\\n",
    "                  replace('-', '').replace(' ', '').replace('&', '').replace('#','').upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'SPARK', 'SPARK', 'IS', 'THE', 'FUTURE,', 'SPARK', 'IS', 'RIGHT', 'NOW']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some ugly characters in there.  Let's create a filter that filters out all tokens that doesn't have at least one letter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "559"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "words = words.filter(lambda x: re.match('[A-Z]+', x))\n",
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FAULTTOLERANT',\n",
       " 'SC',\n",
       " 'SC',\n",
       " 'ASSIGNED',\n",
       " 'LAUNCHES',\n",
       " 'THAT',\n",
       " 'THAT',\n",
       " 'THAT',\n",
       " 'THAT',\n",
       " 'THAT']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parallelize - Distribute a local Python collection to form an RDD\n",
    "words.subtract(sc.parallelize(['IS', 'WHAT'])).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "559"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPARK',\n",
       " 'SPARK',\n",
       " 'IS',\n",
       " 'THE',\n",
       " 'FUTURE,',\n",
       " 'SPARK',\n",
       " 'IS',\n",
       " 'RIGHT',\n",
       " 'NOW',\n",
       " 'THE']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A word counting mapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = words.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SPARK', 1),\n",
       " ('SPARK', 1),\n",
       " ('IS', 1),\n",
       " ('THE', 1),\n",
       " ('FUTURE,', 1),\n",
       " ('SPARK', 1),\n",
       " ('IS', 1),\n",
       " ('RIGHT', 1),\n",
       " ('NOW', 1),\n",
       " ('THE', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SPARK', 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prints out whole RDD, LOTS OF OUTPUT\n",
    "# word_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now do a reduction\n",
    "##### The reduceByKey function\n",
    "- input must be tuples of the form (key, value)\n",
    "- creates a new RDD containing a tuple for each unique value of the key\n",
    "- the value in the output depends upon the supplied lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = word_counts.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FAULTTOLERANT', 1),\n",
       " ('DATAFRAMES', 1),\n",
       " ('LAUNCHES', 1),\n",
       " ('BE', 6),\n",
       " ('THAT', 9),\n",
       " ('SERIALIZABLE', 1),\n",
       " ('WHERE', 2),\n",
       " ('LIKE', 1),\n",
       " ('COLLECTION', 2),\n",
       " ('PROVIDES', 1),\n",
       " ('EXTREMELY', 1),\n",
       " ('PROGRAM))', 1),\n",
       " ('PYTHON', 3),\n",
       " ('WHEN', 1),\n",
       " ('EFFICIENTLY!', 1),\n",
       " ('SETS', 1),\n",
       " ('THIS', 3),\n",
       " ('TURN', 1),\n",
       " ('MORE', 1),\n",
       " ('NEWER', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now do another map and swap the key and the value in terms of their positions\n",
    "##### Which will make the value the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = word_counts.map(lambda x: (x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = word_counts.sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(28, 'SPARK'),\n",
       " (21, 'THE'),\n",
       " (18, 'IS'),\n",
       " (14, 'IN'),\n",
       " (13, 'FOR'),\n",
       " (13, 'DATA'),\n",
       " (11, 'TO'),\n",
       " (11, 'ON'),\n",
       " (10, 'A'),\n",
       " (9, 'THAT')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='2'></a> Example 2:  Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.parallelize(['Its fun to have fun,','but you have to know how.']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RDD.mapPartitionsWithSplit of ParallelCollectionRDD[36] at parallelize at PythonRDD.scala:423>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.mapPartitionsWithIndex\n",
    "lines.mapPartitionsWithSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rd1 = lines.map(lambda x: x.replace('|', '').\\\n",
    "                replace('.', '').replace('-', '').replace('&', '').replace('#','').upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ITS FUN TO HAVE FUN,', 'BUT YOU HAVE TO KNOW HOW']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rd2 = rd1.flatMap(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ITS', 'FUN', 'TO', 'HAVE', 'FUN,', 'BUT', 'YOU', 'HAVE', 'TO', 'KNOW', 'HOW']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd2.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the tuples required for the reduce step, 1 is the value and this will be counted by the reduce lambda function\n",
    "rd3 = rd2.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rd4 = rd3.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BUT', 1),\n",
       " ('TO', 2),\n",
       " ('HOW', 1),\n",
       " ('YOU', 1),\n",
       " ('ITS', 1),\n",
       " ('FUN,', 1),\n",
       " ('KNOW', 1),\n",
       " ('FUN', 1),\n",
       " ('HAVE', 2)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd4.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use another map function to swap the key, value positionally\n",
    "rd5 = rd4.map(lambda x: (x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'BUT'),\n",
       " (2, 'TO'),\n",
       " (1, 'HOW'),\n",
       " (1, 'YOU'),\n",
       " (1, 'ITS'),\n",
       " (1, 'FUN,'),\n",
       " (1, 'KNOW'),\n",
       " (1, 'FUN'),\n",
       " (2, 'HAVE')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd5.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The function sortByKey does exactly what is says, and sorts the tuples using the key value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rd6 = rd5.sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'TO'),\n",
       " (2, 'HAVE'),\n",
       " (1, 'BUT'),\n",
       " (1, 'HOW'),\n",
       " (1, 'YOU'),\n",
       " (1, 'ITS'),\n",
       " (1, 'FUN,'),\n",
       " (1, 'KNOW'),\n",
       " (1, 'FUN')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd6.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3'></a> Example 3:  Word Count (more complicated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's use the 20 news groups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngd = fetch_20newsgroups(shuffle = True, remove = (\"headers\", \"footers\", \"quotes\"), random_state = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mrd_one = sc.parallelize(ngd.data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mrd_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n\\n\\nTheir should be no difference in the drive itself between IBM-PC and Mac.\\nThe two main differences are the formatting of the disk itself (but with\\nthe correct software each can read the others) and maybe the cable\\n(depends on your SCSI board on IBM-PC).\\n\\nIf you get some Mac softawre to allow mounting of ANY IBM-formatted disk\\nand the correct cable you should br able to mount and read your IBM-PC\\nsyquest.\\n\\ngood luck,\\n\\n--Paul\\n\\n-- \\n  +-------------------------------------------------------------------------+\\n  | Paul Hardwick  |  Technical Consulting  |  InterNet: hardwick@panix.com |\\n  | P.O. Box 1482  |  for MVS (SP/XA/ESA)   |  Voice:    (212) 535-0998     |\\n  | NY, NY 10274   |  and 3rd party addons  |  Fax:      (212) Pending      |\\n  +-------------------------------------------------------------------------+',\n",
       " \"\\n\\nFreedom of speech does not mean that others are compelled to give one\\nthe means to speak publicly.  Some systems have regulations\\nprohibiting the dissemination of racist and bigoted messages from\\naccounts they issue.\\n\\nApparently, that's not the case with virginia.edu, since you are still\\nposting.\"]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrd_one.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `glom()` allows you to treat a partition as an array rather than as a single row at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = mrd_one.glom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prints out LOTS OF OUTPUT, SO I'M COMMENTING OUT\n",
    "#test.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chaining commands\n",
    "##### The aim here is to get a list of sentences\n",
    "1. Use `glom()` to convert the partitions to an array of documents\n",
    "2. Use `map()` to join the array of documents into 1 massive string with documents separated by a space\n",
    "3. Use `flatMap()` to split the massive string by sentence into an array of sentences\n",
    "4. Use `map()` to replace all newlines with '' and make everything lowercase\n",
    "5. Use `map()` to remove all occurrences of \"the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mrd2 = mrd_one.glom().map(lambda x: \" \".join(x)).flatMap(lambda x: x.split('.')).map(lambda x: x.replace('\\n', '').\\\n",
    "    lower()).map(lambda x: x.replace(' the ', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['their should be no difference in drive itself between ibm-pc and mac',\n",
       " 'the two main differences are formatting of disk itself (but withthe correct software each can read others) and maybe cable(depends on your scsi board on ibm-pc)']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrd2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Using the sentences write a mapping function to find all the bigrams\n",
    "- Use a `map()` that splits the sentences (x) into a list of tokens via the `split()` function\n",
    "- Use a `flatMap()` that loops through each list and returns something like `((x[i], x[i+1]), 1)` for all the tokens in `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigrams = mrd2.map(lambda x: x.split()).flatMap(lambda x: [((x[i], x[i+1]), 1) for i in range(len(x)-1)]).reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('to', 'be'), 2947),\n",
       " (('it', 'is'), 2889),\n",
       " (('is', 'a'), 2508),\n",
       " (('i', 'have'), 2174),\n",
       " (('if', 'you'), 2170),\n",
       " (('this', 'is'), 1851),\n",
       " (('of', 'a'), 1591),\n",
       " (('in', 'a'), 1531),\n",
       " (('i', 'am'), 1529),\n",
       " (('is', 'not'), 1490)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out the first 10 bigrams with take()\n",
    "bigrams.takeOrdered(10, key=(lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's count up the number of occurrences for each bigram\n",
    "- Use a `reduceByKey()` to sum up the occurrences\n",
    "- Use a `map()` to exchange the resulting keys with values\n",
    "- Use a `sortByKey()` to sort the results in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try it out here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use a `take()` to print out the top 10 bigrams!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try it out here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark supports the efficient parallel application of map and reduce operations by dividing data up into multiple partitions.\n",
    "- Each partition is replicated across multiple workers running on different nodes in a cluster so that failure of a single worker should not cause the RDD to become unavailable.\n",
    "- Many operations including map and flatMap can be applied independently to each partition, running as concurrent jobs based on the number of available cores. \n",
    "- When processing reduceByKey, Spark will create a number of output partitions based on the *default* paralellism based on the numbers of nodes and cores available to Spark. \n",
    "- Data is effectively reshuffled so that input data from different input partitions with the same key value is passed to the same output partition and combined there using the specified reduce function. \n",
    "- sortByKey is another operation which transforms N input partitions to M output partitions.\n",
    "- The number of partitions generated by the reduce stage can be controlled by supplying the desired number of partitions as an extra parameter to reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new1 = sc.parallelize(ngd.data).glom().map(lambda x: \" \".join(x)).flatMap(lambda x: x.split('.')).\\\n",
    "    map(lambda x: x.replace('\\n', '').lower()).map(lambda x: x.replace('the', '')).map(lambda x: x.split()).\\\n",
    "    flatMap(lambda x: [((x[i], x[i+1]), 1) for i in range(0, len(x)-1)]).\\\n",
    "    reduceByKey(lambda a, b: a + b, numPartitions = 12).\\\n",
    "    map(lambda x: (x[1], x[0])).sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def countPartitions(id, iterator): \n",
    "    c = 0 \n",
    "    for _ in iterator: \n",
    "        c += 1 \n",
    "        yield (id, c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 39666, 1: 43980, 2: 90094, 11: 706648}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new1.mapPartitionsWithIndex(countPartitions).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2954, ('to', 'be')),\n",
       " (2895, ('it', 'is')),\n",
       " (2508, ('is', 'a')),\n",
       " (2178, ('i', 'have')),\n",
       " (2170, ('if', 'you')),\n",
       " (1854, ('this', 'is')),\n",
       " (1591, ('of', 'a')),\n",
       " (1531, ('in', 'a')),\n",
       " (1529, ('i', 'am')),\n",
       " (1496, ('is', 'not'))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4'></a> Example 4:  Numerical (prime numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Spark to find all the primes in any range we specify.  Here's a function that determines if a number is prime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isprime(n):\n",
    "    \"\"\"\n",
    "    check if integer n is a prime\n",
    "    \"\"\"\n",
    "    \n",
    "    # make sure n is a positive integer\n",
    "    n = abs(int(n))\n",
    "    \n",
    "    # 0 and 1 are not primes\n",
    "    if n < 2:\n",
    "        return False\n",
    "    \n",
    "    # 2 is the only even prime number\n",
    "    if n == 2:\n",
    "        return True\n",
    "    \n",
    "    # all other even numbers are not primes\n",
    "    if not n & 1:\n",
    "        return False\n",
    "    \n",
    "    # range starts with 3 and only needs to go up the square root of n\n",
    "    # for all odd numbers\n",
    "    for x in range(3, int(n**0.5)+1, 2):\n",
    "        if n % x == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD of numbers from 0 to 1,000,000\n",
    "# python 2\n",
    "#nums = sc.parallelize(1000000)\n",
    "\n",
    "# python 3\n",
    "nums = sc.parallelize(range(1000000))\n",
    "\n",
    "nums.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `filter()` function allows us to supply a function that returns a boolean and filter an `RDD` by those entries that return True for that function.  Here's how we would use it to return prime numbers less than 1 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primes = nums.filter(isprime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primes.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78498\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of primes in the RDD\n",
    "print(primes.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='5'></a> Example 5:  Logistic Regression with Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib is how Spark does Machine Learning.  It has a variety of (what should be!) familiar algorithms that are optimized to work in a distributed fashion!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1:   Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier  # Stochastic Gradient Descent, linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"spark_data/sample_svm_data.txt\", delimiter = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.520784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.004684</td>\n",
       "      <td>2.000347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.228387</td>\n",
       "      <td>2.228387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1         2    3    4    5         6         7    8         9   \\\n",
       "0   1  0.0  2.520784  0.0  0.0  0.0  2.004684  2.000347  0.0  2.228387   \n",
       "\n",
       "         10   11   12   13   14   15   16  \n",
       "0  2.228387  0.0  0.0  0.0  0.0  0.0  0.0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n"
     ]
    }
   ],
   "source": [
    "predictors = dat.columns.values[1:]\n",
    "print(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dat[predictors]\n",
    "y = dat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.636645962733\n"
     ]
    }
   ],
   "source": [
    "clf_pySGD = SGDClassifier(loss='log', alpha = 0.01, n_iter = 10000)\n",
    "clf_pySGD.fit(X, y)\n",
    "yhat = clf_pySGD.predict(X)\n",
    "print(clf_pySGD.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "Actual             \n",
       "0          101   59\n",
       "1           58  104"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = pd.crosstab(y, yhat, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2:   Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LabeledPoint is a built in Pyspark class (label, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_point(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(\"spark_data/sample_svm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map into key value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed_data = data.map(parse_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parsed_data)\n",
    "parsed_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Use the Spark logisitic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.classification.LogisticRegressionModel"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_clf = LogisticRegressionWithSGD.train(parsed_data)\n",
    "type(spark_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### p is a labelled point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_and_predictions = parsed_data.map(lambda p: (p.label, spark_clf.predict(p.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 1),\n",
      " (0.0, 1),\n",
      " (0.0, 0),\n",
      " (1.0, 1),\n",
      " (1.0, 0),\n",
      " (0.0, 1),\n",
      " (1.0, 1),\n",
      " (1.0, 1),\n",
      " (0.0, 0),\n",
      " (0.0, 0)]\n"
     ]
    }
   ],
   "source": [
    "type(labels_and_predictions)\n",
    "pprint(labels_and_predictions.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322\n"
     ]
    }
   ],
   "source": [
    "print(parsed_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 59), (1.0, 104)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yyhat = labels_and_predictions.reduceByKey(lambda x, y: x + y).collect()\n",
    "yyhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 59), (1.0, 104)]\n",
      "[(0, 58.0), (1, 104.0)]\n"
     ]
    }
   ],
   "source": [
    "yyhat = labels_and_predictions \\\n",
    "  .reduceByKey(lambda x, y: x + y) \\\n",
    "  .collect()\n",
    "landp = labels_and_predictions.map(lambda x : (x[1], x[0]))\n",
    "yyhat_1 = landp.reduceByKey(lambda x, y: x + y).collect()\n",
    "\n",
    "print(yyhat)\n",
    "print(yyhat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "print(labels_and_predictions.filter(lambda x: x[0] != x[1]).count())\n",
    "print(labels_and_predictions.filter(lambda x: x[0] == x[1]).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = list(labels_and_predictions.take(1000))\n",
    "y = np.array([x[0] for x in results])\n",
    "yhat = np.array([x[1] for x in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>101</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>58</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "Actual             \n",
       "0.0        101   59\n",
       "1.0         58  104"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = pd.crosstab(y, yhat, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36335403726708076\n"
     ]
    }
   ],
   "source": [
    "training_error = labels_and_predictions.filter(lambda x: x[0] != x[1]).count()/float(parsed_data.count())\n",
    "type(training_error)\n",
    "print(training_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Your Own\n",
    "Go back to any of your favorite classification datasets that we've dealt with and see if you can implement the classifier with Spark as we just did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
