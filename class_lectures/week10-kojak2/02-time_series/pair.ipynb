{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is the same as merge in Pandas and inner join in SQL. One soluton: loop through list1, and for each element in list1 you loop through every element in list2. That's a double for-loop and the complexity of such a code would be **O(N^2)**.\n",
    "\n",
    "Instead we could build a dictionary for list1. Then we will loop through the elements of list2 and join that way. The complexity would be **O(N)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_join(list1, list2):\n",
    "    dict1 = {item[0]:item[1:] for item in list1}\n",
    "    result = list()\n",
    "    \n",
    "    for item in list2:\n",
    "        if item[0] in dict1:\n",
    "            result.append([item[0]] + dict1[item[0]] + item[1:])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general though, the dictionary building part is done prior to running the join/merge query. That step is called **indexing**. A dictionary type index is called **hash table index**. There are other indexing used, like b-tree. Each has its advantages and disadvantages. We'll be looking at that tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inner_join(list1, list2):\n",
    "    dict1 = {item[0]:item[1:] for item in list1}\n",
    "    dict2 = {item[0]:item[1:] for item in list2}\n",
    "    \n",
    "    result = list()\n",
    "    \n",
    "    for key, value in dict1.items():\n",
    "        if key in dict2:\n",
    "            result.append([key] + dict1[key] + dict2[key])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below is a little routine to test the performance of the functions. You can use it to test your code. Try varying out the lists, dictionaries and list comprehensions and see what the difference in performance is. Of course, it will also depend on the speed of your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(127)\n",
    "numbers = range(1000)\n",
    "random.shuffle(numbers)\n",
    "test1 = [[index] + [random.random() for _ in range(100)] for index in numbers]\n",
    "random.shuffle(numbers)\n",
    "test2 = [[index] + [random.random() for _ in range(100)] for index in numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 3.75 ms per loop\n",
      "100 loops, best of 3: 4.98 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit my_join(test1, test2)\n",
    "%timeit inner_join(test1, test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, the functions above assumed the indexes are unique in each of the lists. Which, as we know is not usually the case. We can change the function to handle duplicates by storing a list-of-lists for each dictionary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def my_join(list1, list2):\n",
    "    dict1 = defaultdict(list)\n",
    "    for item in list1:\n",
    "        dict1[item[0]].append(item[1:])\n",
    "    result = []\n",
    "    for item in list2:\n",
    "        if item[0] in dict1:\n",
    "            for elem in dict1[item[0]]:\n",
    "                result.append([item[0]] + elem + item[1:])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 7], [2, 20, 7], [1, 2, 4], [2, 3, 11], [2, 20, 11]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_join([[1,2],[2,3],[2,20]], [[2,7],[1,4],[2,11]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity of function above is still O(N) when the indexes are unique or mostly unique. The worst case is when all the index values in both lists are the same. In that case, the join will provide N^2 rows. So there is no way to avoid O(N^2)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
