{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Setup \n",
    "\n",
    "We will be using Docker again today. You'll want to start the AWS EC2 instance that you were using yesterday. \n",
    "\n",
    "Note that the IP address will usually change when an instance is restarted on AWS.\n",
    "\n",
    "## Setup\n",
    "\n",
    "There is a little bit of devops setup to be completed during the pair exercise as well. Getting comfortable with ssh and scp is a really important devops skill!\n",
    "\n",
    "You'll want to `scp` the notebooks [Spark_Recommendation_Systems.ipynb](Spark_Recommendation_Systems.ipynb) and [Spark_Spam_Classification.ipynb](Spark_Spam_Classification.ipynb) into the notebooks directory (`/home/ubuntu/notebooks`) on AWS. Recall that this directory is shared with Docker, so we will be able to access this notebook with Jupyter running on Docker.\n",
    "\n",
    "\n",
    "Note that you will have to insert your IP address:\n",
    "\n",
    "```console\n",
    "scp -i ~/.ssh/id_rsa Spark_Spam_Classification.ipynb ubuntu@xx.xx.xxx:~/notebooks\n",
    "scp -i ~/.ssh/id_rsa Spark_Recommendation_Systems.ipynb ubuntu@xx.xx.xxx:~/notebooks\n",
    "```\n",
    "\n",
    "Also copy the two data files from this repo. Assuming the `scp` command is being run from this directory, the commands are below. Note that the data directory was created yesterday on your AWS instance when we ran the classification notebook.\n",
    "\n",
    "```console\n",
    "scp -i ~/.ssh/id_rsa ../../../resources/spark/spark_data/spam_train.csv ubuntu@xx.xx.xxx:~/notebooks/data\n",
    "scp -i ~/.ssh/id_rsa ../../../resources/spark/spark_data/spam_test.csv ubuntu@xx.xx.xxx:~/notebooks/data\n",
    "```  \n",
    "\n",
    "Also ensure the docker container is running type typing the command `docker ps` after you've logged into your AWS instance. If it's not, then it can be restarted as before:\n",
    "\n",
    "```console\n",
    "docker run -d -p 8888:8888 -v /home/ubuntu/notebooks:/home/ubuntu/notebooks metisbootcamp/metis-spark-python:latest\n",
    "```  \n",
    "\n",
    "There's no need to pull the Docker image again.\n",
    "\n",
    "## Start SSH Tunnel\n",
    "\n",
    "Lastly, start the SSH tunnel from your laptop:\n",
    "\n",
    "```bash\n",
    "ssh -i ~/.ssh/id_rsa -NL 12345:localhost:8888 ubuntu@XX.XXX.XXX.XXX\n",
    "```\n",
    "\n",
    "Browse to http://localhost:12345 on your laptop and you should see the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "When you are finished, all notebooks, data, and output (plots, etc.) can be copied from AWS using `scp`. You should terminate the instance or (at a minimum) stop it to minimize fees incurred on AWS EC2."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
