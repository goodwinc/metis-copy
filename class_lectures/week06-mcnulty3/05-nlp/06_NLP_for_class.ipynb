{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Motivation\n",
    "\n",
    "We often have text data on people, reviews, etc., and we want to predict something about them - such as whether a person will click on an ad, or whether a review is positive or negative.\n",
    "\n",
    "![](http://soph.info/metis/blackrock_python/sample_text_data.png)\n",
    "\n",
    "The question is then: how can we extract useful information from this text data that we can then use to make predictions? As text data typically represents a lot of words, characters and so on, a key question will also be: how do we get rid of data that _doesn't_ matter? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working today with the Cereal data, which has reviews of cookies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "# <-- Run this if it's your first time using nltk to download all of the datasets and models\n",
    "# nltk.download() \n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we explore a new dataset, it's good to start by getting a quick feel for what we're working with. In this case we'll read the data file to a pandas dataframe, look at the first few rows, and look at the first 3 complete reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A368Z46FIKHSEZ</td>\n",
       "      <td>5</td>\n",
       "      <td>I love these cookies!  Not only are they healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JAPP1CXRG57A</td>\n",
       "      <td>5</td>\n",
       "      <td>Quaker Soft Baked Oatmeal Cookies with raisins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2Z9JNXPIEL2B9</td>\n",
       "      <td>5</td>\n",
       "      <td>I am usually not a huge fan of oatmeal cookies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A31CYJQO3FL586</td>\n",
       "      <td>5</td>\n",
       "      <td>I participated in a product review that includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2KXQ2EKFF3K2G</td>\n",
       "      <td>5</td>\n",
       "      <td>My kids loved these. I was very pleased to giv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  stars                                            reviews\n",
       "0  A368Z46FIKHSEZ      5  I love these cookies!  Not only are they healt...\n",
       "1  A1JAPP1CXRG57A      5  Quaker Soft Baked Oatmeal Cookies with raisins...\n",
       "2  A2Z9JNXPIEL2B9      5  I am usually not a huge fan of oatmeal cookies...\n",
       "3  A31CYJQO3FL586      5  I participated in a product review that includ...\n",
       "4  A2KXQ2EKFF3K2G      5  My kids loved these. I was very pleased to giv..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('cookie_reviews.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each row of this data corresponds to one user, with two interesting columns that we'll focus on:\n",
    "\n",
    "* **stars**: rating the user gave the cookie\n",
    "* **reviews**: text of the user's review\n",
    "\n",
    "Our ultimate task in this notebook will be to use NLP to extract text features from the reviews column, then use these features to predict the review's rating.\n",
    "\n",
    "From running the cell below, notice that reviews can vary greatly in length and that some reviews contain messy looking 'br' tags. We'll want to clean these up later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love these cookies!  Not only are they healthy but they taste great and are so soft!  I will definitely add these to my grocery list! \n",
      "\n",
      "Quaker Soft Baked Oatmeal Cookies with raisins are a delicious treat, great for anytime of day.  For example:<br /><br />--at breakfast, I had one with a large banana and a cup of coffee, and felt I'd had a relatively \"healthy\" start to the day.<br /><br />--the next day at lunch, following a tuna sandwich, I had one with a glass of milk, and was satisfied enough to not need a snack before dinner at 6:30.<br /><br />--the following night, after dinner, I had one with the remainder of my glass of wine. (Delicious!) And again, didn't feel the need to snack later in the evening.<br /><br />Each cookie is individually packaged, and their texture is soft and moist, with just the right amount of sweetness. Natural flavors used in the making are Cinnamon and All Spice.  These flavorings give the cookies a real old-fashioned, homemade taste.<br /><br />Nutritionally, the cookies have 170 calories each, 1.5g saturated fat, 150 mg sodium, and 12g sugar. They also have 2g of protein, and contain 25g of fiber.<br /><br />While the calorie count may seem a bit high for one cookie, they are good sized, and 1 cookie per serving is certainly enough to satisfy.<br /><br />Because of their great taste and texture, kids will probably enjoy them also.<br /><br />If you like oatmeal raisin cookies, give these a try! \n",
      "\n",
      "I am usually not a huge fan of oatmeal cookies, but these literally melt in your mouth. They are so soft and tasty! I would definitely recommend these to someone who loves oatmeal, and even those like me who would probably pick a different flavor over this one. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for review in data['reviews'][:3]:\n",
    "    print(review, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd like to build out our high-level understanding of the data set by understanding some key summary statistics. We want to:\n",
    "\n",
    "* Determine how many reviews there are in total.\n",
    "* Determine the percent of 1, 2, 3, 4 and 5 star reviews.\n",
    "* Determine the distribution of character lengths for the reviews, by listing the values and by plotting a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 913 reviews total\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.683461\n",
       "4    0.237678\n",
       "3    0.061336\n",
       "2    0.013143\n",
       "1    0.004381\n",
       "Name: stars, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cookie got great reviews overall, with 68% of users giving it 5 stars\n",
    "data.stars.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the character length of each review by calling .str.len() on the reviews column, which applies the string length function to each review. \n",
    "\n",
    "After that, we can visualize this result as a histogram using matplotlib/seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     135\n",
       "1    1316\n",
       "2     261\n",
       "3     530\n",
       "4     169\n",
       "Name: reviews, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_lengths = data.reviews.str.len()\n",
    "review_lengths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEXCAYAAABRWhj0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1d96f668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import seaborn for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# This creates a histogram of all of the review lengths\n",
    "ax = review_lengths.hist(bins=50)\n",
    "ax.set(xlabel='Review Length', ylabel='Frequency', \n",
    "       title='Distribution of Cookie Review Length');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're comfortable with the dataset now, so we're ready to dive into some real NLP.\n",
    "\n",
    "There are usually several steps to the process of cleaning natural language data into a useful form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Turn text into a meaningful format for analysis**\n",
    "    * Tokenization: split each document into a list of \"tokens\", e.g. single words\n",
    "        - This gives us a basis for numerical comparison across documents, e.g. counting word occurences\n",
    "\n",
    "2. **Clean the data**\n",
    "    * Remove: capital letters, punctuation, numbers, very common words (i.e. \"stop words\") \n",
    "    * Stemming, parts of speech tagging\n",
    "    * Chunking: named entity recognition, compound term extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use many of these as we transform our reviews data into a form where it is ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tokenization = splitting raw text into smaller units for processing ** \n",
    "\n",
    "These units can be:\n",
    "* Words \n",
    "* Sentences\n",
    "* Characters\n",
    "* N-grams: 2 or more word or character pairs, e.g. ('baseball','bat) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK library makes some common tokenization tasks easy. Let's say you wanted to break this sentence (which we'll use as our example sentence throughout) into words that are useful for modeling:\n",
    "\n",
    "> \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up 2lbs of black-eyed peas as well?\"\n",
    "\n",
    "How would you do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Mr.', 'Smith', ',', 'hi', '!', 'I', '’', 'm', 'going', 'to', 'buy', 'some', 'vegetables', '(', 'tomatoes', 'and', 'cucumbers', ')', 'from', 'the', 'store', '.', 'Should', 'I', 'pick', 'up', '2lbs', 'of', 'black-eyed', 'peas', 'as', 'well', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "my_text = \"Hi Mr. Smith, hi! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up 2lbs of black-eyed peas as well?\"\n",
    "print(word_tokenize(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the problems with this? To answer this question, let's briefly cover how we'd use these as features in a model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural language features in machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text_df = pd.DataFrame(data=[my_text], index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for token in word_tokenize(my_text):\n",
    "    text_df[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Hi</th>\n",
       "      <th>Mr.</th>\n",
       "      <th>Smith</th>\n",
       "      <th>,</th>\n",
       "      <th>hi</th>\n",
       "      <th>!</th>\n",
       "      <th>I</th>\n",
       "      <th>’</th>\n",
       "      <th>m</th>\n",
       "      <th>...</th>\n",
       "      <th>Should</th>\n",
       "      <th>pick</th>\n",
       "      <th>up</th>\n",
       "      <th>2lbs</th>\n",
       "      <th>of</th>\n",
       "      <th>black-eyed</th>\n",
       "      <th>peas</th>\n",
       "      <th>as</th>\n",
       "      <th>well</th>\n",
       "      <th>?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi Mr. Smith, hi! I’m going to buy some vegeta...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  Hi  Mr.  Smith  ,  hi  \\\n",
       "0  Hi Mr. Smith, hi! I’m going to buy some vegeta...   1    1      1  1   1   \n",
       "\n",
       "   !  I  ’  m ...  Should  pick  up  2lbs  of  black-eyed  peas  as  well  ?  \n",
       "0  1  1  1  1 ...       1     1   1     1   1           1     1   1     1  1  \n",
       "\n",
       "[1 rows x 34 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we're going to create features for whether each of these words is present in each review. In NLP terminology, each row of our dataset will be a **document** (review), and each column will be a **term** (occurence of a word or other token).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example of how we could add term features for every _bigram_ in the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'Mr.'), ('Mr.', 'Smith'), ('Smith', ','), (',', 'hi'), ('hi', '!'), ('!', 'I'), ('I', '’'), ('’', 'm'), ('m', 'going'), ('going', 'to'), ('to', 'buy'), ('buy', 'some'), ('some', 'vegetables'), ('vegetables', '('), ('(', 'tomatoes'), ('tomatoes', 'and'), ('and', 'cucumbers'), ('cucumbers', ')'), (')', 'from'), ('from', 'the'), ('the', 'store'), ('store', '.'), ('.', 'Should'), ('Should', 'I'), ('I', 'pick'), ('pick', 'up'), ('up', '2lbs'), ('2lbs', 'of'), ('of', 'black-eyed'), ('black-eyed', 'peas'), ('peas', 'as'), ('as', 'well'), ('well', '?')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "my_words = word_tokenize(my_text) # This is the list of all words\n",
    "twograms = list(ngrams(my_words,2)) # This is for two-word combos, but can pick any n\n",
    "print(twograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for two_gram in twograms:\n",
    "    text_df[two_gram] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Hi</th>\n",
       "      <th>Mr.</th>\n",
       "      <th>Smith</th>\n",
       "      <th>,</th>\n",
       "      <th>hi</th>\n",
       "      <th>!</th>\n",
       "      <th>I</th>\n",
       "      <th>’</th>\n",
       "      <th>m</th>\n",
       "      <th>...</th>\n",
       "      <th>(Should, I)</th>\n",
       "      <th>(I, pick)</th>\n",
       "      <th>(pick, up)</th>\n",
       "      <th>(up, 2lbs)</th>\n",
       "      <th>(2lbs, of)</th>\n",
       "      <th>(of, black-eyed)</th>\n",
       "      <th>(black-eyed, peas)</th>\n",
       "      <th>(peas, as)</th>\n",
       "      <th>(as, well)</th>\n",
       "      <th>(well, ?)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi Mr. Smith, hi! I’m going to buy some vegeta...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  Hi  Mr.  Smith  ,  hi  \\\n",
       "0  Hi Mr. Smith, hi! I’m going to buy some vegeta...   1    1      1  1   1   \n",
       "\n",
       "   !  I  ’  m    ...      (Should, I)  (I, pick)  (pick, up)  (up, 2lbs)  \\\n",
       "0  1  1  1  1    ...                1          1           1           1   \n",
       "\n",
       "   (2lbs, of)  (of, black-eyed)  (black-eyed, peas)  (peas, as)  (as, well)  \\\n",
       "0           1                 1                   1           1           1   \n",
       "\n",
       "   (well, ?)  \n",
       "0          1  \n",
       "\n",
       "[1 rows x 67 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words vs. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach - looking at whether certain words or bigrams are present in a text document - is known as a **bag of words** approach.\n",
    "\n",
    "The other approach is one you may have heard of: **TF-IDF** or **Term Frequency-Inverse Document Frequency**. We will discuss this in more detail later; suffice it to say, here we would look at how common a word is in a document _relative to how common it is overall_, so that words that are common in _all_ documents like \"the\" are not given too much weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now: given the level of cleaning we've done and the features we've generated: if we created these features for every review in our \"cookies\" dataset, would the features be likely to help us in predicting the review's rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Hi</th>\n",
       "      <th>Mr.</th>\n",
       "      <th>Smith</th>\n",
       "      <th>,</th>\n",
       "      <th>hi</th>\n",
       "      <th>!</th>\n",
       "      <th>I</th>\n",
       "      <th>’</th>\n",
       "      <th>m</th>\n",
       "      <th>...</th>\n",
       "      <th>(Should, I)</th>\n",
       "      <th>(I, pick)</th>\n",
       "      <th>(pick, up)</th>\n",
       "      <th>(up, 2lbs)</th>\n",
       "      <th>(2lbs, of)</th>\n",
       "      <th>(of, black-eyed)</th>\n",
       "      <th>(black-eyed, peas)</th>\n",
       "      <th>(peas, as)</th>\n",
       "      <th>(as, well)</th>\n",
       "      <th>(well, ?)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi Mr. Smith, hi! I’m going to buy some vegeta...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  Hi  Mr.  Smith  ,  hi  \\\n",
       "0  Hi Mr. Smith, hi! I’m going to buy some vegeta...   1    1      1  1   1   \n",
       "\n",
       "   !  I  ’  m    ...      (Should, I)  (I, pick)  (pick, up)  (up, 2lbs)  \\\n",
       "0  1  1  1  1    ...                1          1           1           1   \n",
       "\n",
       "   (2lbs, of)  (of, black-eyed)  (black-eyed, peas)  (peas, as)  (as, well)  \\\n",
       "0           1                 1                   1           1           1   \n",
       "\n",
       "   (well, ?)  \n",
       "0          1  \n",
       "\n",
       "[1 rows x 67 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features will give us a lot of useful information about the reviews' content, **but notice that some of our tokens are probably useless**:\n",
    "\n",
    "* Punctuation marks like !\n",
    "* Repeat tokens from case differences: \"Hi\" and \"hi\"  \n",
    "* Specific numerical quantities like \"2lbs\"\n",
    "* Extremely common words like \"to\", \"the\"\n",
    "\n",
    "Additionally, once we tokenize many documents we'll likely get different tokens for different forms of the same word, e.g. \"run\" vs. \"running\", \"runs\", \"ran\". We'll want to standardize these different forms into one token. \n",
    "\n",
    "We'd like to clean up these 5 types of extraneous tokens to get a smaller and more impactful feature set. We'll see how to do this for each type below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets remove punctuation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do this by leveraging the ```str.maketrans``` function, which gives us a dictionary mapping every punctuation character to the empty string. We can then apply this mapping to our text by using ```translate```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Mr Smith hi I’m going to buy some vegetables tomatoes and cucumbers from the store Should I pick up 2lbs of blackeyed peas as well'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "remove_punct = str.maketrans('', '', string.punctuation)\n",
    "clean_text_1 = my_text.translate(remove_punct)\n",
    "clean_text_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example review from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quaker Soft Baked Oatmeal Cookies with raisins are a delicious treat, great for anytime of day.  For example:<br /><br />--at breakfast, I had one with a large banana and a cup of coffee, and felt I'd had a relatively \"healthy\" start to the day.<br /><br />--the next day at lunch, following a tuna sandwich, I had one with a glass of milk, and was satisfied enough to not need a snack before dinner at 6:30.<br /><br />--the following night, after dinner, I had one with the remainder of my glass of wine. (Delicious!) And again, didn't feel the need to snack later in the evening.<br /><br />Each cookie is individually packaged, and their texture is soft and moist, with just the right amount of sweetness. Natural flavors used in the making are Cinnamon and All Spice.  These flavorings give the cookies a real old-fashioned, homemade taste.<br /><br />Nutritionally, the cookies have 170 calories each, 1.5g saturated fat, 150 mg sodium, and 12g sugar. They also have 2g of protein, and contain 25g of fiber.<br /><br />While the calorie count may seem a bit high for one cookie, they are good sized, and 1 cookie per serving is certainly enough to satisfy.<br /><br />Because of their great taste and texture, kids will probably enjoy them also.<br /><br />If you like oatmeal raisin cookies, give these a try!\n"
     ]
    }
   ],
   "source": [
    "example_review = data.iloc[1,2]\n",
    "print(example_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation from it and store the result in a variable called `example_review_punct`.\n",
    "\n",
    "**Bonus:** Wrap this in a function called `remove_punct` that takes in a string and removes punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we might want to do is make all the text lowercase, which is easy to do with the ```lower``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi mr smith hi i’m going to buy some vegetables tomatoes and cucumbers from the store should i pick up 2lbs of blackeyed peas as well'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_2 = clean_text_1.lower()\n",
    "clean_text_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our third cleaning step, we want to remove numbers from the text. Similarly to how we removed punctuation, we can use the ```translate``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import digits\n",
    "\n",
    "digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi mr smith hi i’m going to buy some vegetables tomatoes and cucumbers from the store should i pick up lbs of blackeyed peas as well'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "clean_text_3 = clean_text_2.translate(remove_digits)\n",
    "clean_text_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we word tokenize this, we get a much cleaner result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'mr', 'smith', 'hi', 'i', '’', 'm', 'going', 'to', 'buy', 'some', 'vegetables', 'tomatoes', 'and', 'cucumbers', 'from', 'the', 'store', 'should', 'i', 'pick', 'up', 'lbs', 'of', 'blackeyed', 'peas', 'as', 'well']\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = word_tokenize(clean_text_3)\n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the two cleaning steps from above:\n",
    "\n",
    "* Make each word in the review lowercase\n",
    "* Remove numbers\n",
    "\n",
    "Then, word tokenize the string into a list called `example_review_clean`. \n",
    "\n",
    "**Bonus:** Wrap this in a function called `lower_and_remove_numbers` that takes in a string and turns the words to lowercase, removes the numbers, and performs word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Our tokens are clean now, but we haven't yet addressed the problem of extremely common words, or **stop words**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why care about stop words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most common word in this sentence:\n",
    "\n",
    "> \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up 2lbs of black-eyed peas as well?\"\n",
    "\n",
    "Turns out it is \"some\". \n",
    "\n",
    "**Is the fact that the word \"some\" appears in this review very informative?** Probably not! When we build a machine learning model, we would prefer to exclude word features that just take up space without giving us any predictive value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, \"some\" is an example of a stop word, and NLTK has a way to deal with it in the form of a collection of stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "clean_tokens_stop = [y for y in clean_tokens if y not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'mr', 'smith', 'hi', '’', 'going', 'buy', 'vegetables', 'tomatoes', 'cucumbers', 'store', 'pick', 'lbs', 'blackeyed', 'peas', 'well']\n"
     ]
    }
   ],
   "source": [
    "print(clean_tokens_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking even better now!\n",
    "\n",
    "If you're curious about what all the stopwords in this collection are, here they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'what', 'about', 'any', 'yourself', 'it', 'those', 'my', 'but', 'to', 'when', 'off', 'himself', 'how', \"shouldn't\", 'so', \"that'll\", 'aren', \"mightn't\", \"you've\", 'ma', 'itself', 'just', 'ain', 'by', \"wouldn't\", 'during', 'd', \"doesn't\", \"weren't\", \"it's\", 'hasn', 'didn', \"haven't\", 'no', \"hasn't\", \"needn't\", 'this', 'between', 'be', \"you'll\", 'further', 'and', 'more', 'who', 'in', 'herself', 'her', 'because', 'again', 'doesn', 'which', 'couldn', 'as', 'now', 'hadn', 'they', 'where', 'y', 'theirs', \"isn't\", 'very', 'over', 'few', 'don', 'did', 'above', 'will', 'nor', 'then', 'with', 'own', 'from', 'being', 'down', 'she', 'through', 'm', 'before', 'here', 'mustn', 'does', 't', 'll', 'are', 'shouldn', 'into', 'than', 'is', 'we', 'your', 'wasn', 'our', 'against', 'after', 'such', 's', 'themselves', 'while', 'if', 'ours', 'o', 'wouldn', 'an', 'a', 'for', \"aren't\", \"mustn't\", 'until', 'myself', 'on', 'weren', 'were', 'some', \"shan't\", 'ourselves', 'too', 'once', 'his', 'its', 'can', 'the', 'mightn', \"you're\", 'them', 'or', 'had', 'won', 'yours', 'doing', 'below', 'having', 'have', 'why', 've', \"won't\", 'each', 'these', 'do', 'only', \"couldn't\", 'haven', 'their', 'up', 'you', 'that', 'there', \"hadn't\", 'needn', 'should', 'shan', 'both', 'out', 'isn', \"she's\", \"didn't\", 'been', 'yourselves', 'other', 'hers', \"wasn't\", 'was', 'i', 're', \"should've\", 'whom', \"don't\", 'he', 'him', 'of', 'most', 'all', 'under', 'am', 'me', 'not', 'same', 'at', 'has', \"you'd\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Remove stop words from `example_review_clean`, and store the result in a list called `example_review_stop`.\n",
    "\n",
    "**Bonus:** Wrap this in a function called `remove_stop_words` that takes in a string of tokens of words and removes the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about go vs. going? Should those be considered differently in our model? Probably not. Enter stemming and lemmatization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://soph.info/metis/blackrock_python/stem_lem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll focus on stemming. There are a number of stemming options built into nltk, including Lancaster and Snowball stemming. For now, we won't worry about the differences between these two and will just see how they are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, here's an example of how stemming can standardize different forms of the same root word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive: driv\n",
      "drives: driv\n",
      "driver: driv\n",
      "drivers: driv\n",
      "driven: driv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "# Try some stems\n",
    "print('drive: {}'.format(stemmer.stem('drive')))\n",
    "print('drives: {}'.format(stemmer.stem('drives')))\n",
    "print('driver: {}'.format(stemmer.stem('driver')))\n",
    "print('drivers: {}'.format(stemmer.stem('drivers')))\n",
    "print('driven: {}'.format(stemmer.stem('driven')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can stem our actual text tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform stemming, which removes alternate word endings without knowledge of the context\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "clean_tokens_stem = [stemmer.stem(y) for y in clean_tokens_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'mr', 'smith', 'hi', '’', 'go', 'buy', 'veget', 'tomato', 'cucumb', 'store', 'pick', 'lbs', 'blackey', 'pea', 'well']\n"
     ]
    }
   ],
   "source": [
    "print(clean_tokens_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Perform stemming on `example_review_stop`, and store the result in a list called `example_review_stem`.\n",
    "\n",
    "**Bonus:** Wrap this in a function called `stem_list` that takes in a string of tokens of words and performs stemming on each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, our tokens look much cleaner now than what we started with! At this point, we've worked through the most commonly used processing steps in NLP applications in Machine Learning. There are many other common tasks such as part of speech tagging, but these don't have as direct an application to extracting valuable information from data for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will take a string called `input_string` and turn it into a DataFrame with unigrams and bigrams as features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def unigrams_bigrams(input_string):\n",
    "    text_df = pd.DataFrame(data=[input_string], index=[0])\n",
    "    for token in word_tokenize(input_string):\n",
    "        text_df[token] = 1\n",
    "\n",
    "    my_words = word_tokenize(input_string) \n",
    "    twograms = list(ngrams(my_words,2)) \n",
    "\n",
    "    for two_gram in twograms:\n",
    "        text_df[two_gram] = 1\n",
    "            \n",
    "    return text_df\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run this code with our original review string - `example_review` and see how many features result.\n",
    "\n",
    "2. Then, run this code with our \"cleaned\" review string - `example_review_stem` - and see how many features result.\n",
    "\n",
    "Which has more features? Why? Which do you think will be more effective and interpretable as input to a machine learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other preprocessing could be helpful? What other features might we be able to extract that could help us distinguish between the reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the cookies review data\n",
    "\n",
    "Now that we've learned these 5 key text preprocessing techniques, let's practice them by applying them to the cookies review data in a series of exercises. Remember to reference the steps above to figure out how to solve each part! To make it easier, we've copied the old relevant code next to each exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A368Z46FIKHSEZ</td>\n",
       "      <td>5</td>\n",
       "      <td>I love these cookies!  Not only are they healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JAPP1CXRG57A</td>\n",
       "      <td>5</td>\n",
       "      <td>Quaker Soft Baked Oatmeal Cookies with raisins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2Z9JNXPIEL2B9</td>\n",
       "      <td>5</td>\n",
       "      <td>I am usually not a huge fan of oatmeal cookies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A31CYJQO3FL586</td>\n",
       "      <td>5</td>\n",
       "      <td>I participated in a product review that includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2KXQ2EKFF3K2G</td>\n",
       "      <td>5</td>\n",
       "      <td>My kids loved these. I was very pleased to giv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  stars                                            reviews\n",
       "0  A368Z46FIKHSEZ      5  I love these cookies!  Not only are they healt...\n",
       "1  A1JAPP1CXRG57A      5  Quaker Soft Baked Oatmeal Cookies with raisins...\n",
       "2  A2Z9JNXPIEL2B9      5  I am usually not a huge fan of oatmeal cookies...\n",
       "3  A31CYJQO3FL586      5  I participated in a product review that includ...\n",
       "4  A2KXQ2EKFF3K2G      5  My kids loved these. I was very pleased to giv..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the first together. Note the handy use of ```data['reviews'].str``` to apply a string-based function to every record in the reviews column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the code used to clean the punctuation previously was:\n",
    "\n",
    "```python\n",
    "remove_punct = str.maketrans('', '', string.punctuation)\n",
    "clean_text_1 = my_text.translate(remove_punct)\n",
    "clean_text_1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I love these cookies  Not only are they health...\n",
       "1    Quaker Soft Baked Oatmeal Cookies with raisins...\n",
       "2    I am usually not a huge fan of oatmeal cookies...\n",
       "3    I participated in a product review that includ...\n",
       "4    My kids loved these I was very pleased to give...\n",
       "Name: reviews, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXERCISE 1: create a new pandas series that stores \n",
    "# all of the cookie reviews with punctuation removed.  \n",
    "\n",
    "remove_punct = str.maketrans('', '', string.punctuation)\n",
    "reviews_clean_1 = data['reviews'].str.translate(remove_punct)\n",
    "reviews_clean_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCISE 2: take the result of ex 1 (punctuation removed),\n",
    "# and convert the reviews to lower case and remove numbers. Use .str\n",
    "# like in the exercise above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PREVIOUS CODE:\n",
    "\n",
    "```python\n",
    "clean_text_2 = clean_text_1.lower()\n",
    "clean_text_2\n",
    "\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "clean_text_3 = clean_text_2.translate(remove_digits)\n",
    "clean_text_3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCISE 3: tokenize the result of ex 2  \n",
    "\n",
    "# HINT: you can apply a function like word_tokenize to each entry \n",
    "# in a pandas series by calling .map(function_name) on the series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PREVIOUS CODE:\n",
    "\n",
    "```python    \n",
    "clean_tokens = word_tokenize(clean_text_3)\n",
    "print(clean_tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCISE 4: take the result of ex 3 (list of tokens) and \n",
    "# remove stop words\n",
    "\n",
    "# HINT: turn the previous code into a function that removes stop words \n",
    "# from 1 token list, then use map to apply it to all \n",
    "# collections of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PREVIOUS CODE:\n",
    "\n",
    "```python    \n",
    "stop_words = stopwords.words('english')\n",
    "clean_tokens_stop = [y for y in clean_tokens if y not in stop_words]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCISE 5: take the result of ex 4 (list of stopword removed tokens) \n",
    "# and stem all the tokens using the SnowballStemmer\n",
    "\n",
    "# HINT: same approach as exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PREVIOUS CODE:\n",
    "\n",
    "```python\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "clean_tokens_stem = [stemmer.stem(y) for y in clean_tokens_stop]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, now we've gotten to a list of preprocessed tokens for the entire set of reviews. Since at this point we're comfortable with NLP preprocessing. Now's let's show how these NLP techniques can help us predict things about these reviews using machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building machine learning models, the very first thing we should do is to separate the data into a **training set** and a **test set**. This way, we can fit a model to the training set, and evaluate how well it might perform on new, unseen data by scoring it on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = data.reviews, data.stars\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=.3, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((639,), (639,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape # number of reviews we train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((274,), (274,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape # number of reviews we test on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've separated our data into train and test, let's try to understand how we can turn word tokens into numerical features. Remember the simple example we saw earlier for our toy sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Hi</th>\n",
       "      <th>Mr.</th>\n",
       "      <th>Smith</th>\n",
       "      <th>,</th>\n",
       "      <th>hi</th>\n",
       "      <th>!</th>\n",
       "      <th>I</th>\n",
       "      <th>’</th>\n",
       "      <th>m</th>\n",
       "      <th>...</th>\n",
       "      <th>(Should, I)</th>\n",
       "      <th>(I, pick)</th>\n",
       "      <th>(pick, up)</th>\n",
       "      <th>(up, 2lbs)</th>\n",
       "      <th>(2lbs, of)</th>\n",
       "      <th>(of, black-eyed)</th>\n",
       "      <th>(black-eyed, peas)</th>\n",
       "      <th>(peas, as)</th>\n",
       "      <th>(as, well)</th>\n",
       "      <th>(well, ?)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi Mr. Smith, hi! I’m going to buy some vegeta...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  Hi  Mr.  Smith  ,  hi  \\\n",
       "0  Hi Mr. Smith, hi! I’m going to buy some vegeta...   1    1      1  1   1   \n",
       "\n",
       "   !  I  ’  m    ...      (Should, I)  (I, pick)  (pick, up)  (up, 2lbs)  \\\n",
       "0  1  1  1  1    ...                1          1           1           1   \n",
       "\n",
       "   (2lbs, of)  (of, black-eyed)  (black-eyed, peas)  (peas, as)  (as, well)  \\\n",
       "0           1                 1                   1           1           1   \n",
       "\n",
       "   (well, ?)  \n",
       "0          1  \n",
       "\n",
       "[1 rows x 67 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply have 1 column for each term, with a 1 marking that the term has occured in this document.\n",
    "\n",
    "**CountVectorizer** is only a bit more complicated than this. CountVectorizer collects a **vocabulary** of all of the tokens that occur across all documents, creating a column for each vocabulary term.\n",
    "\n",
    "For each document (row), the value stored in the term column will be the **number of times that term occurs in the document**. So in essence we're just counting how many times a word occurs in each review. Luckily, sci-kit learn has functionality to do this for us automatically, even handling tokenization for us!\n",
    "\n",
    "Here is a nice example on a small collection of 5 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chai</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>encoding</th>\n",
       "      <th>have</th>\n",
       "      <th>hot</th>\n",
       "      <th>is</th>\n",
       "      <th>latte</th>\n",
       "      <th>make</th>\n",
       "      <th>milk</th>\n",
       "      <th>my</th>\n",
       "      <th>one</th>\n",
       "      <th>sale</th>\n",
       "      <th>sun</th>\n",
       "      <th>the</th>\n",
       "      <th>there</th>\n",
       "      <th>today</th>\n",
       "      <th>under</th>\n",
       "      <th>weather</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chai  chocolate  encoding  have  hot  is  latte  make  milk  my  one  sale  \\\n",
       "0     0          0         0     0    1   1      0     0     0   0    0     0   \n",
       "1     0          1         0     0    1   0      0     1     1   1    0     0   \n",
       "2     0          0         1     0    1   0      0     0     0   0    1     0   \n",
       "3     1          0         0     1    0   0      1     0     1   0    0     0   \n",
       "4     0          0         0     0    1   1      0     0     0   0    0     1   \n",
       "\n",
       "   sun  the  there  today  under  weather  will  with  \n",
       "0    1    2      0      0      1        1     0     0  \n",
       "1    0    0      0      0      0        0     0     1  \n",
       "2    0    0      0      0      0        0     0     0  \n",
       "3    0    0      0      0      0        0     1     1  \n",
       "4    0    0      1      1      0        0     0     0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['The weather is hot under the sun',\n",
    "          'I make my hot chocolate with milk',\n",
    "          'One hot encoding',\n",
    "          'I will have a chai latte with milk',\n",
    "          'There is a hot sale today']\n",
    "\n",
    "# create the document-term matrix with count vectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "dt = pd.DataFrame(X, columns=cv.get_feature_names())\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since most of the vocabulary words do not occur in any given document, most of the feature column values are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying CountVectorizer to the cookies review data\n",
    "\n",
    "With that introduction to CountVectorizer, let's run it on our real dataset. We already have a train and test split, so we just need to run the CountVectorizer transformations to extract features before fitting and scoring a model.\n",
    "\n",
    "We'll use the simple **Naive Bayes** model, a commonly used benchmark model for text problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# get training data vocabulary and extract term count column results \n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "\n",
    "# use training data vocabulary to extract term count column results \n",
    "X_test_cv  = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7262773722627737"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(X_train_cv, y_train)\n",
    "nb.score(X_test_cv, y_test) # accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decent, more than 72% accurate just using a naive countvectorizer.\n",
    "\n",
    "To bring together the preprocessing we learned earlier with our machine learning, we'll enhance the CountVectorizer we're using by improving its tokenization. We'll use all the steps we learned earlier.\n",
    "\n",
    "It turns out that we can do this by defining a **custom tokenizer function** as below. This function will take a text string as input, cleaning and tokenizing it as desired to produce a token list as output. To make CountVectorizer use this tokenizer, all we have to do is pass the function to the argument ```tokenizer``` when creating our vectorizer. That tells it to use our function to tokenize every document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "\n",
    "    # remove punctuation\n",
    "    remove_punct = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(remove_punct)\n",
    "\n",
    "    # remove digits and convert to lower case\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    text = text.lower().translate(remove_digits)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens_stop = [y for y in tokens if y not in stop_words]\n",
    "\n",
    "    # stem\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokens_stem = [stemmer.stem(y) for y in tokens_stop] \n",
    "\n",
    "    return tokens_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7299270072992701"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countvectorizing and training\n",
    "cv = CountVectorizer(tokenizer=custom_tokenizer)\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv  = cv.transform(X_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(X_train_cv, y_train)\n",
    "nb.score(X_test_cv, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our custom tokenizer, the accuracy result is a bit better.\n",
    "\n",
    "But it's possible that we may have been overzealous in our cleaning, removing some information that's actually useful for predicting the rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCISE 6: Try making some adjustments to our custom tokenizer above \n",
    "# and rerunning the countvectorize + train step above\n",
    "# to see if you can improve the score. \n",
    "\n",
    "# What if we skip the stemming stage at the end? Try removing/commenting\n",
    "# out the stemming steps and/or just returning tokens_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCISE 7: Let's explore the options built into CountVectorizer a\n",
    "# bit more. Open up the documentation by running the cell below, \n",
    "# and read about the ngram_range argument.\n",
    "\n",
    "# Uncomment the code below and adjust it to use both 1-gram (single word) \n",
    "# and 2-gram tokens. Are the test results better or worse than before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STARTER CODE:\n",
    "\n",
    "```python\n",
    "cv = CountVectorizer(ngram_range=FILL IN)\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv  = cv.transform(X_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(X_train_cv, y_train)\n",
    "nb.score(X_test_cv, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tf-idf Vectorizer\n",
    "\n",
    "To wrap up, we'll briefly explore a more advanced term feature extraction technique: **Term Frequency - Inverse Document Frequency**.\n",
    "\n",
    "Though it sounds fancy, it's really only slightly more complicated than CountVectorizer. We still tokenize and extract term counts exactly as before, but adjust the term counts to normalize for how common terms are across documents.\n",
    "\n",
    "For example, the word \"cookie\" likely occurs in very many reviews, while the word \"delicious\" is probably more rare. Accordingly, when we see \"cookie\" we should downweight it for being common and uninformative, giving \"delicious\" more relative importance because its occurence is more informative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://soph.info/metis/blackrock_python/tf_idf_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://soph.info/metis/blackrock_python/tf_idf_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustration of `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chai</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>encoding</th>\n",
       "      <th>have</th>\n",
       "      <th>hot</th>\n",
       "      <th>is</th>\n",
       "      <th>latte</th>\n",
       "      <th>make</th>\n",
       "      <th>milk</th>\n",
       "      <th>my</th>\n",
       "      <th>one</th>\n",
       "      <th>sale</th>\n",
       "      <th>sun</th>\n",
       "      <th>the</th>\n",
       "      <th>there</th>\n",
       "      <th>today</th>\n",
       "      <th>under</th>\n",
       "      <th>weather</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199581</td>\n",
       "      <td>0.285811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354256</td>\n",
       "      <td>0.708511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354256</td>\n",
       "      <td>0.354256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465281</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465281</td>\n",
       "      <td>0.375386</td>\n",
       "      <td>0.465281</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.434297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.434297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434297</td>\n",
       "      <td>0.350388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282814</td>\n",
       "      <td>0.405004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.501992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501992</td>\n",
       "      <td>0.501992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       chai  chocolate  encoding      have       hot        is     latte  \\\n",
       "0  0.000000   0.000000    0.0000  0.000000  0.199581  0.285811  0.000000   \n",
       "1  0.000000   0.465281    0.0000  0.000000  0.262131  0.000000  0.000000   \n",
       "2  0.000000   0.000000    0.6569  0.000000  0.370086  0.000000  0.000000   \n",
       "3  0.434297   0.000000    0.0000  0.434297  0.000000  0.000000  0.434297   \n",
       "4  0.000000   0.000000    0.0000  0.000000  0.282814  0.405004  0.000000   \n",
       "\n",
       "       make      milk        my     one      sale       sun       the  \\\n",
       "0  0.000000  0.000000  0.000000  0.0000  0.000000  0.354256  0.708511   \n",
       "1  0.465281  0.375386  0.465281  0.0000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.6569  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.350388  0.000000  0.0000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.0000  0.501992  0.000000  0.000000   \n",
       "\n",
       "      there     today     under   weather      will      with  \n",
       "0  0.000000  0.000000  0.354256  0.354256  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.375386  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.434297  0.350388  \n",
       "4  0.501992  0.501992  0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['The weather is hot under the sun',\n",
    "          'I make my hot chocolate with milk',\n",
    "          'One hot encoding',\n",
    "          'I will have a chai latte with milk',\n",
    "          'There is a hot sale today']\n",
    "\n",
    "# create the document-term matrix with count vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "dt = pd.DataFrame(X, columns=tfidf.get_feature_names())\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to CountVectorizer, we can do this automatically using sklearn. Note that the only real difference between this code and what we already did is that we use TfidfVectorizer instead of CountVectorizer - very convenient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7007299270072993"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "nb.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, Tf-idf Vectorizer does not seem to work better than the simpler CountVectorizer. In many cases it does work better, but this reinforces an important lesson in machine learning: there's no such thing as a model or feature type that's always superior. Sometimes simpler is better! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
