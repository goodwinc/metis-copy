{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This morning we'll implement a method for featurizing text, i.e. turning words into numeric values that can be fed to machine learning models. We'll use a small set of amazon reviews with positive/negative sentiment labels (balanced classes), and a simple multinomial naive bayes model to test our method.\n",
    "\n",
    "You can use sklearn for everything except the Count Vectorizer methods :)\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "**1.** Read in the data, take a quick look, and do a train/test split. This one's a gift to you:\n",
    "\n",
    "```\n",
    "df = pd.read_csv(\"Data/amazon_cells_labelled.txt\", sep='\\t', names=['text', 'sentiment'])\n",
    "\n",
    "df.head()\n",
    "```\n",
    "\n",
    "```\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.sentiment,\n",
    "                                                    test_size=.2, random_state=2018)\n",
    "```    \n",
    "\n",
    "**2.** We want to predict the sentiment of the review only using the text. ??? How do we turn words into numeric features? \n",
    "\n",
    "This is where the **Count Vectorizer** method comes into play. It turns out we can do something pretty simple - just count word occurences across all of our different text samples (documents). Each word in the the entire corpus (collection of documents) gets its own feature column. \n",
    "\n",
    "Your major task is to write a class and/or series of functions that accomplish the following:\n",
    "\n",
    "* Iterate through a corpus and collect all of the distinct words that occur into a global **vocabulary**. Hint: try using Counter from the collections library.\n",
    "\n",
    "* To each word in the vocabulary, assign a consistent ordered position - for example, you could sort by the number of occurences (but any consistent positioning is fine).\n",
    "\n",
    "* **Transform a corpus into a numeric dataframe**, with one column for each word in the vocabulary. For each document in the corpus (row in the dataframe), count occurences of each word and fill the corresponding dataframe columns with the appropriate counts. The positioning in bullet 2 allows you to do this consistently. This output is called a **document-term matrix**. \n",
    "\n",
    "**3. ** Once you've built your Count Vectorizer, apply it to the review data. Build your vocabulary off of the entire corpus (df.text), and convert the train and test corpuses (X_train, X_test) to document-term matrices using your transform function. Congrats, you now have numeric features and targets! Fit a multinomial naive bayes model to the train data and score it for accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  So there is no way for me to plug it in here i...          0\n",
       "1                        Good case, Excellent value.          1\n",
       "2                             Great for the jawbone.          1\n",
       "3  Tied to charger for conversations lasting more...          0\n",
       "4                                  The mic is great.          1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"Data/amazon_cells_labelled.txt\", sep='\\t', names=['text', 'sentiment'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.sentiment,\n",
    "                                                    test_size=.2, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class My_CountVectorizer:\n",
    "    \n",
    "    def fit_vocab(self, corpus):\n",
    "        \n",
    "        # collect vocabulary word counts\n",
    "        self.counter = Counter()\n",
    "        for doc in corpus:\n",
    "            self.counter.update(doc.split(' '))\n",
    "        \n",
    "        # map words to columns in descending order of frequency\n",
    "        self.feature_map = {word : i for i, (word, count) \n",
    "                            in enumerate(self.counter.most_common())}\n",
    "           \n",
    "    def transform_corpus(self, corpus):\n",
    "        \n",
    "        vectors = []\n",
    "        \n",
    "        # fill doc rows by iterating through words and \n",
    "        # accumulating counts to term columns\n",
    "        for doc in corpus:\n",
    "            vector = np.zeros(len(self.feature_map))\n",
    "            for word in doc.split(' '):\n",
    "                vector[self.feature_map[word]] += 1\n",
    "            vectors.append(vector)\n",
    "        \n",
    "        # document-term matrix with word column names\n",
    "        word_df = pd.DataFrame(vectors, columns=self.feature_map.keys())\n",
    "        return word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = My_CountVectorizer()\n",
    "cv.fit_vocab(df.text)\n",
    "X_train = cv.transform_corpus(X_train)\n",
    "X_test = cv.transform_corpus(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>and</th>\n",
       "      <th>I</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>to</th>\n",
       "      <th>it</th>\n",
       "      <th>this</th>\n",
       "      <th>my</th>\n",
       "      <th>of</th>\n",
       "      <th>...</th>\n",
       "      <th>Match</th>\n",
       "      <th>Picture.</th>\n",
       "      <th>disappoint</th>\n",
       "      <th>infra</th>\n",
       "      <th>red</th>\n",
       "      <th>port</th>\n",
       "      <th>(irda).</th>\n",
       "      <th>answer</th>\n",
       "      <th>unit,</th>\n",
       "      <th>once!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   the  and    I   is    a   to   it  this   my   of  ...    Match  Picture.  \\\n",
       "0  0.0  1.0  1.0  0.0  0.0  0.0  2.0   0.0  0.0  0.0  ...      0.0       0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...      0.0       0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...      0.0       0.0   \n",
       "3  0.0  1.0  0.0  1.0  0.0  0.0  0.0   0.0  0.0  1.0  ...      0.0       0.0   \n",
       "4  0.0  1.0  1.0  0.0  2.0  1.0  0.0   0.0  2.0  1.0  ...      0.0       0.0   \n",
       "\n",
       "   disappoint  infra  red  port  (irda).  answer  unit,  once!  \n",
       "0         0.0    0.0  0.0   0.0      0.0     0.0    0.0    0.0  \n",
       "1         0.0    0.0  0.0   0.0      0.0     0.0    0.0    0.0  \n",
       "2         0.0    0.0  0.0   0.0      0.0     0.0    0.0    0.0  \n",
       "3         0.0    0.0  0.0   0.0      0.0     0.0    0.0    0.0  \n",
       "4         0.0    0.0  0.0   0.0      0.0     0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 2815 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn\n",
    "\n",
    "Sklearn lets us get everything we just wrote automatically! And it's better, because we can expect it to be more efficient and it uses better preprocessing out of the box (we'll cover a bunch of the core preprocessing steps soon). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.805"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# same train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.sentiment,\n",
    "                                                    test_size=.2, random_state=2018)\n",
    "\n",
    "X_train = cv.fit_transform(X_train)\n",
    "X_test = cv.transform(X_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
