{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why Twitter ?\n",
    "\n",
    "\n",
    "\"While physics and math may tell us how the universe began, they are not much use in predicting Human Behavior because there are far too many Equations to Solve\" \n",
    "    \n",
    "    -Stephan Hawking\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of other great APIs out there as well that you should keep in mind.\n",
    "\n",
    "[Wikipedia](https://www.mediawiki.org/wiki/API:Main_page)  \n",
    "[Stack exchange](https://api.stackexchange.com/docs)  \n",
    "[Spotify](http://spotipy.readthedocs.io/en/latest/)\n",
    "\n",
    "Often there's already a python package with a much easier interface for using the API, like spotipy that's linked to above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rest API vs Streaming API: \n",
    "\n",
    "\n",
    "REST:  \n",
    "    - Query user accounts using OAuth\n",
    "    - Allows you to access 'historical' tweets\n",
    "    - Only lets you access tweets going back 7 days\n",
    "\n",
    "STREAM: \n",
    "    - Essentially long-running request (left Open) using OAuth\n",
    "    - Access realtime stream of data\n",
    "       \n",
    "Check out the [twitter developer documentation](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rest API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install requests_oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "\n",
    "#OAuth ~ simple way to to publish & interact with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install cnfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing our Config - note that with this setup we can safely conceal\n",
    "# our personal keys but still share generalizable code\n",
    "\n",
    "# Replace the path below with the correct path to your config file\n",
    "import cnfg\n",
    "config = cnfg.load(\"Documents/twitter_api/twitter_config\")\n",
    "\n",
    "oauth = OAuth1(config[\"consumer_key\"],\n",
    "               config[\"consumer_secret\"],\n",
    "               config[\"access_token\"],\n",
    "               config[\"access_token_secret\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.twitter.com/1.1/statuses/user_timeline.json\",\n",
    "                        auth=oauth)\n",
    "\n",
    "tweets = response.json()\n",
    "\n",
    "for key in tweets[0].keys():\n",
    "    print(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I only have one tweet (I'm not really a twitter user )\n",
    "for tweet in tweets:\n",
    "    print(tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the twitter API to search for 5 tweets that are about our favorite topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"q\": \"gradient boosting\", \"count\":5}\n",
    "response = requests.get(\"https://api.twitter.com/1.1/search/tweets.json\",\n",
    "                        params = parameters,\n",
    "                        auth=oauth)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(response.json()['search_metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = response.json()['statuses']\n",
    "\n",
    "print('PAGE 1')\n",
    "for tweet in tweets:\n",
    "    print(tweet['id'], tweet['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = \"https://api.twitter.com/1.1/search/tweets.json\"\n",
    "next_page_url = search_url + response.json()['search_metadata']['next_results']\n",
    "\n",
    "response = requests.get(next_page_url, auth=oauth)\n",
    "\n",
    "print('PAGE 2')\n",
    "for tweet in response.json()['statuses']:\n",
    "    print(tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STREAMING API ~ TWEEPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(config[\"consumer_key\"],\n",
    "                           config[\"consumer_secret\"])\n",
    "auth.set_access_token(config[\"access_token\"],\n",
    "                      config[\"access_token_secret\"])\n",
    "\n",
    "api=tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweets=1\n",
    "\n",
    "#Tweepy Cursor handles pagination .. \n",
    "\n",
    "for tweet in tweepy.Cursor(api.search,q=\"data science\").items(max_tweets):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results=[]\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search,q=\"gradient boosting\").items(10):\n",
    "    results.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import tweets into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  import pandas as pd\n",
    "def structure_results(results):\n",
    "    id_list=[tweet.id for tweet in results]\n",
    "    data=pd.DataFrame(id_list,columns=['id'])\n",
    "    \n",
    "    data[\"text\"]= [tweet.text.encode('utf-8') for tweet in results]\n",
    "    data[\"datetime\"]=[tweet.created_at for tweet in results]\n",
    "    data[\"Location\"]=[tweet.place for tweet in results]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=structure_results(results)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on tweepy and using it to set up a true stream, check out the [tweepy doc](http://docs.tweepy.org/en/v3.4.0/streaming_how_to.html) as well as this [nice guide from dataquest](https://www.dataquest.io/blog/streaming-data-python/). \n",
    "\n",
    "Combining these steps with writing to a mongo database as below, you can setup a live data acquisition pipeline that continually grabs data from twitter as tweets hit and stores them locally for later static analysis/modeling. We'll show an example at the very bottom of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Tweets into MongoDB\n",
    "\n",
    "Install mongo locally with brew:    \n",
    "```\n",
    "brew update    \n",
    "brew install mongodb\n",
    "```\n",
    "After downloading mongo, we want to create a place for mongo data files to live.  Run:    \n",
    "```\n",
    "sudo mkdir -p /data/db\n",
    "```\n",
    "Make sure that /data/db directory has the right permissions:\n",
    "\n",
    "```\n",
    "sudo chown `active_username` /data/db\n",
    "(enter password) \n",
    "```\n",
    "\n",
    "(username ~ is just mac username (you can double\n",
    "check this by running 'whoami' in the terminal)\n",
    "\n",
    "\n",
    "Run mongo daemon:\n",
    "```\n",
    "mongod\n",
    "```\n",
    "\n",
    "(In order to access direct mongo functionality, you can just\n",
    "run 'mongo' in a separate terminal)\n",
    "\n",
    "Once the mongo daemon is running, we can create a **database** \"example\" with a **collection** \"datascience\" as below. Note that the database and collection only exist once we insert tweets into them, since mongo is lazy by design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "client = MongoClient(port=27017)\n",
    "db = client.example\n",
    "tweets = db.datascience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in results:\n",
    "    data={}\n",
    "    data['tweet']=tweet.text.encode('utf-8') \n",
    "    data['datetime']=tweet.created_at\n",
    "    tweets.insert_one(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here is a quick example of pulling it all together into a live data acquisition feed/store. This will run continually and grab new tweets as they hit if it doesn't error, so let's leave it for about a minute and see what we get. You should have mongod still running for this.\n",
    "\n",
    "If you want to use this for a project, you'll want to add some robustness to the stream listener - check out the tweepy doc for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(port=27017)\n",
    "db = client.example_live\n",
    "livetweets = db.datascience\n",
    "\n",
    "livetweets.drop() # only if we want to start from scratch\n",
    "\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "\n",
    "    def on_status(self, status):\n",
    "        \n",
    "        # skip retweets\n",
    "        if status.retweeted: \n",
    "            return\n",
    "    \n",
    "        # store tweet and creation date\n",
    "        data = {}\n",
    "        data['tweet'] = status.text\n",
    "        data['datetime'] = status.created_at\n",
    "\n",
    "        # insert into db\n",
    "        try:\n",
    "            livetweets.insert_one(data)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "stream_listener = StreamListener()\n",
    "stream = tweepy.Stream(auth=api.auth, listener=stream_listener)\n",
    "stream.filter(track=['data science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livetweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livetweets.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
