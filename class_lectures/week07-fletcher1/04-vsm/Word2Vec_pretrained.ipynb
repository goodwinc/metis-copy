{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using 2 well-known text datasets to explore the capabilities of Word2Vec:\n",
    "- [Spam Classification Dataset](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/): Collection of SMS text messages labeled as Spam/Not Spam (Ham)\n",
    "- [20 Newsgroups Dataset](http://qwone.com/~jason/20Newsgroups/): Famous text classification dataset from user discussion forums with 20 classes, including...\n",
    "  - Computer Graphics (comp.graphics)\n",
    "  - Microsoft Windows (comp.os.ms-windows.misc)\n",
    "  - IBM Hardware (comp.sys.ibm.pc.hardware)\n",
    "  - Mac Hardware (comp.sys.mac.hardware)\n",
    "  - Windows XP (comp.windows.x)\n",
    "  - For Sale (misc.forsale)\n",
    "  - Automobiles (rec.autos)\n",
    "  - Motorcycles (rec.motorcycles)\n",
    "  - Baseball (rec.sport.baseball)\n",
    "  - Hockey (rec.sport.hockey)\n",
    "  - General Politics (talk.politics.misc)\n",
    "  - Politics - Gun Control (talk.politics.guns)\n",
    "  - Middle East Politics (talk.politics.mideast)\n",
    "  - Cryptography (sci.crypt)\n",
    "  - Electronics (sci.electronics)\n",
    "  - Medicine (sci.med)\n",
    "  - Space (sci.space)\n",
    "  - Religion (talk.religion.misc)\n",
    "  - Atheism (alt.atheism)\n",
    "  - Christianity (soc.religion.christian)\n",
    "  \n",
    "To perform our tasks, we will both derive our own **word vectors** from the data as well as borrow Google's massive set of word vectors trained on the web ([Google Vectors](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)). Download these and put them under nltk data - note that this file is pretty large!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:54:14.154063Z",
     "start_time": "2017-07-27T21:54:09.408786Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLP tools\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "# Data tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Necessary for adding accessory_functions module to path\n",
    "import os, sys\n",
    "lib_path = os.path.abspath(os.path.join('..', '..'))\n",
    "sys.path.append(lib_path)\n",
    "from accessory_functions import google_vec_file, nltk_path\n",
    "\n",
    "# Python 2 compatibility\n",
    "from __future__ import print_function\n",
    "\n",
    "# Setup nltk corpora path\n",
    "nltk.data.path.insert(0, nltk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "**Loading Google Word2Vec Vectors**\n",
    "* Load the Google vectors into an object `google_model` using `gensim` (This step will take awhile, as it has to load 3 million vectors into the appropriate Word2Vec format).\n",
    "* Confirm that you have 3 million vectors of length 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:57:16.564833Z",
     "start_time": "2017-07-27T21:54:14.156571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the Google vectors\n",
    "google_model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google's model contains an extensive vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:57:16.599932Z",
     "start_time": "2017-07-27T21:57:16.568339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(google_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:57:16.616373Z",
     "start_time": "2017-07-27T21:57:16.603226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Vectors\n",
    "len(google_model.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of the Vectors\n",
    "google_model.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "**Exploring Word2Vec Vectors**\n",
    "* Print out a few word vectors from the Google set\n",
    "* Print out the similarity between the following pairs (feel free to experiment with more if you like):\n",
    "  * baseball, bat\n",
    "  * baseball, ocean\n",
    "  * bat, fly\n",
    "* What sorts of patterns do you notice?  Where does it succeed?  Where does it fail?  How might one improve it?\n",
    "* Print out the most similar words to the following words:\n",
    "  * baseball\n",
    "  * president\n",
    "* Print out words similar to the positive words and dissimilar to the negative words for the following positive/negative groups:\n",
    "* Print out the words that don't match the others in each of the following groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10546875,  0.25585938,  0.25390625, -0.03125   , -0.28515625,\n",
       "       -0.1953125 ,  0.18261719, -0.06298828,  0.18457031,  0.08007812,\n",
       "       -0.07324219, -0.25585938, -0.12792969,  0.08251953, -0.046875  ,\n",
       "       -0.08984375,  0.07714844,  0.3515625 ,  0.26367188, -0.13867188,\n",
       "       -0.125     ,  0.09130859, -0.01062012, -0.140625  , -0.04321289,\n",
       "        0.15917969, -0.07763672,  0.34179688,  0.12255859,  0.30273438,\n",
       "        0.04956055,  0.140625  , -0.0378418 ,  0.04052734, -0.01831055,\n",
       "        0.07128906,  0.29492188, -0.17773438,  0.2578125 ,  0.18652344,\n",
       "        0.18261719, -0.07128906,  0.22558594, -0.05932617,  0.05566406,\n",
       "        0.17773438,  0.1875    , -0.1875    , -0.05664062,  0.32421875,\n",
       "       -0.05297852,  0.00236511, -0.08398438, -0.0123291 , -0.03088379,\n",
       "       -0.16015625, -0.01043701, -0.06835938, -0.01257324, -0.16601562,\n",
       "       -0.11376953, -0.0612793 ,  0.13183594,  0.07324219,  0.23730469,\n",
       "        0.00515747, -0.23535156,  0.13378906,  0.2734375 ,  0.08740234,\n",
       "        0.34179688,  0.02490234, -0.10205078, -0.18847656, -0.01281738,\n",
       "        0.16015625,  0.21582031, -0.04931641, -0.04711914, -0.22167969,\n",
       "       -0.19628906, -0.13183594, -0.17675781,  0.09277344, -0.13378906,\n",
       "       -0.07177734, -0.359375  ,  0.32617188,  0.11376953, -0.1796875 ,\n",
       "       -0.14160156, -0.04516602,  0.08007812,  0.07128906, -0.13476562,\n",
       "        0.21289062,  0.30078125,  0.20117188, -0.10839844, -0.15039062,\n",
       "       -0.10058594,  0.0378418 ,  0.18652344, -0.16796875,  0.38476562,\n",
       "        0.28320312,  0.12402344, -0.05419922, -0.26757812, -0.05859375,\n",
       "        0.16503906,  0.09472656, -0.23144531,  0.13378906,  0.3828125 ,\n",
       "        0.65234375, -0.13183594,  0.08740234,  0.00160217, -0.00958252,\n",
       "       -0.171875  , -0.00799561, -0.1015625 , -0.10888672, -0.03344727,\n",
       "       -0.30273438, -0.13867188,  0.28125   , -0.29882812, -0.06884766,\n",
       "       -0.03540039, -0.40820312, -0.11132812,  0.07226562, -0.04492188,\n",
       "       -0.16015625, -0.23046875,  0.02392578, -0.1640625 , -0.18359375,\n",
       "       -0.15039062,  0.12304688,  0.2109375 , -0.13085938,  0.05175781,\n",
       "        0.28710938,  0.109375  , -0.09619141,  0.05859375, -0.32421875,\n",
       "        0.03320312, -0.01489258, -0.45117188, -0.02429199, -0.33203125,\n",
       "       -0.17382812,  0.14648438, -0.17675781, -0.09423828,  0.01635742,\n",
       "       -0.02624512, -0.09082031,  0.00793457, -0.00540161, -0.02160645,\n",
       "       -0.12158203,  0.05786133, -0.33007812, -0.07763672,  0.02478027,\n",
       "        0.08398438, -0.05395508,  0.1875    , -0.20117188,  0.08349609,\n",
       "       -0.01867676,  0.10302734,  0.16894531, -0.11767578,  0.16699219,\n",
       "        0.00720215,  0.16699219, -0.06689453, -0.07421875,  0.04467773,\n",
       "        0.06884766, -0.09765625, -0.18457031,  0.265625  ,  0.01733398,\n",
       "        0.15625   ,  0.15820312, -0.10595703, -0.01263428, -0.33203125,\n",
       "        0.05615234,  0.01000977, -0.25195312, -0.10498047, -0.07568359,\n",
       "       -0.09472656, -0.26367188, -0.12109375, -0.12890625, -0.22070312,\n",
       "       -0.328125  ,  0.12988281, -0.27929688, -0.07421875,  0.00909424,\n",
       "       -0.08447266, -0.02026367, -0.1875    ,  0.16601562, -0.11035156,\n",
       "        0.22753906, -0.16113281,  0.06884766, -0.12353516,  0.012146  ,\n",
       "        0.08544922, -0.3984375 , -0.16699219, -0.02844238,  0.23242188,\n",
       "        0.11083984, -0.00104523,  0.11914062,  0.02966309, -0.10595703,\n",
       "        0.07373047,  0.03417969, -0.03198242,  0.03271484, -0.00170898,\n",
       "       -0.01611328,  0.04321289, -0.27734375,  0.32617188, -0.1796875 ,\n",
       "        0.11279297, -0.2890625 ,  0.2265625 , -0.37304688, -0.16992188,\n",
       "       -0.0112915 , -0.13183594, -0.17871094, -0.13964844, -0.05981445,\n",
       "       -0.02172852,  0.00543213,  0.13964844, -0.05126953,  0.03271484,\n",
       "        0.11914062,  0.06152344, -0.34375   , -0.10693359,  0.07714844,\n",
       "        0.296875  , -0.18554688,  0.08203125,  0.12304688, -0.20898438,\n",
       "        0.36914062, -0.20898438, -0.07128906, -0.20214844, -0.12792969,\n",
       "        0.00448608,  0.16015625, -0.21972656,  0.12695312, -0.01489258,\n",
       "       -0.2734375 , -0.02038574,  0.06738281,  0.17675781, -0.24414062,\n",
       "        0.06445312,  0.12011719,  0.28125   ,  0.24121094, -0.1171875 ,\n",
       "       -0.05859375, -0.31640625,  0.20117188, -0.0559082 , -0.14160156,\n",
       "       -0.11767578,  0.02539062, -0.10546875, -0.13964844,  0.2734375 ,\n",
       "        0.10253906,  0.06103516,  0.05737305, -0.01434326,  0.10986328], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Vectors\n",
    "google_model.word_vec('baseball')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62116989722645277"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pairwise Similarity\n",
    "google_model.similarity('Bill_Clinton', 'Barack_Obama')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Obama', 0.8036513328552246),\n",
       " ('Barrack_Obama', 0.7766816020011902),\n",
       " ('Illinois_senator', 0.757197916507721),\n",
       " ('McCain', 0.7530534267425537),\n",
       " ('Barack', 0.7448184490203857),\n",
       " ('Barack_Obama_D-Ill.', 0.7196038961410522),\n",
       " ('Hillary_Clinton', 0.6864978671073914),\n",
       " ('Sen._Hillary_Clinton', 0.6827855110168457),\n",
       " ('elect_Barack_Obama', 0.6812860369682312),\n",
       " ('Clinton', 0.6713167428970337)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most similar words\n",
    "google_model.similar_by_word('Barack_Obama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192315101624),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Positive Negative Similar Words\n",
    "google_model.most_similar(positive=['Barack_Obama', 'Bill_Clinton'], negative=['president'])\n",
    "google_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'President'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words that don't match\n",
    "google_model.doesnt_match(['Mitt_Romney', 'Ted_Cruz', 'Hillary_Clinton', 'John_McCain', 'President'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "**Document Vectors from Word Vectors**\n",
    "* Compare the following sample documents to each other by cosine similarity\n",
    "* Compare the same set of documents together by Word Mover's Distance  \n",
    "\n",
    "***Note***: You will need to take care to first do the following:\n",
    "  * Split the documents into lists of words.\n",
    "  * Remove all words that aren't in the Google vector vocabulary (Word2Vec errors otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72718535909185844"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing via Cosine Similarity\n",
    "google_model.n_similarity(['Barack_Obama', 'president'], ['Bill_Clinton', 'President'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyemd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.283404423946738"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing via Word Mover's Distance\n",
    "google_model.wmdistance(['Barack_Obama', 'president'], ['Bill_Clinton', 'President'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Features\n",
    "\n",
    "Each word contains an array of 300 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:57:16.623990Z",
     "start_time": "2017-07-27T21:57:16.618537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(google_model.word_vec('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:57:16.635938Z",
     "start_time": "2017-07-27T21:57:16.627563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n",
       "        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656,\n",
       "        0.08056641, -0.5859375 , -0.00445557, -0.296875  , -0.01312256,\n",
       "       -0.08349609,  0.05053711,  0.15136719, -0.44921875, -0.0135498 ], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.word_vec('cat')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity between words can be computed and produces intuitive trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:57:16.644153Z",
     "start_time": "2017-07-27T21:57:16.637670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.760945708978\n",
      "0.215281850364\n"
     ]
    }
   ],
   "source": [
    "print(google_model.similarity('cat', 'cat'))\n",
    "print(google_model.similarity('cat', 'dog'))\n",
    "print(google_model.similarity('cat', 'car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:57:16.650736Z",
     "start_time": "2017-07-27T21:57:16.646207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.673579016963\n",
      "0.313944691624\n"
     ]
    }
   ],
   "source": [
    "print(google_model.similarity('car', 'truck'))\n",
    "print(google_model.similarity('car', 'drive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec captures some interesting similarities between words, such as the relationship between **man --> king** and **woman --> queen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:57:57.113878Z",
     "start_time": "2017-07-27T21:57:16.652764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192315101624),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8887498780166667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.wmdistance(\"Obama is the president of the United States\".lower().split(), \n",
    "                        \"Bush was the president of the United States\".lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: Why does the previous command take so much longer than others?\n",
    "Because it has to generate a new vector that is **woman** + **king** - **man** and compare that vector to all 3 million vectors, then sort to find the closest 3.  The 3 million are stored in such a way that they can be compared quickly, but any new vector is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also detect words that don't belong in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:57:57.146451Z",
     "start_time": "2017-07-27T21:57:57.122781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "**Training a Word2Vec Model**\n",
    "* Read the spam dataset into a Dataframe.\n",
    "* Preprocess the documents using our previous `accesory_functions`.\n",
    "* Split each document into a list of words.\n",
    "* Use the results to train your own word2vec model with `gensim`.\n",
    "\n",
    "##### Question: What does \"training a word2vec model\" mean?\n",
    "Answer: generating good word vectors, or \"word embeddings\" for all unique words in the text dataset.  \"Good\" means similar words have their vectors cluster close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  go jurong point crazy available bugis n great ...\n",
       "1   ham                            ok lar joking wif u oni\n",
       "2  spam  free entry wkly comp win fa cup final tkts may...\n",
       "3   ham                u dun say early hor u c already say\n",
       "4   ham                nah think go usf life around though"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from accessory_functions import preprocess_series_text, nltk_path\n",
    "\n",
    "spam_data = pd.read_csv('data/spam.csv', sep='\\t', header=None)\n",
    "spam_data.columns = ['label','text']\n",
    "spam_data['text'] = preprocess_series_text(spam_data.text, nltk_path=nltk_path)\n",
    "\n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the Word2Vec model.  `gensim` requires the documents to be represented as a ***list of sentences*** to train Word2Vec.  Here we'll do this by calling `split()` on our document text to turn each document into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Generate sentences for training word2vec\n",
    "sentences = spam_data.text.str.split()\n",
    "# Train a Word2Vec model\n",
    "spam_model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out some conceptual comparisons with our word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('please', 0.9998553991317749),\n",
       " ('customer', 0.9998331665992737),\n",
       " ('number', 0.9998165369033813),\n",
       " ('service', 0.9998107552528381),\n",
       " ('reply', 0.9998052716255188),\n",
       " ('holiday', 0.9998008608818054),\n",
       " ('text', 0.999797523021698),\n",
       " ('show', 0.9997959733009338),\n",
       " ('line', 0.9997928738594055),\n",
       " ('live', 0.999782383441925)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_model.most_similar('call')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but if you investigate deeper you realize the results are so-so at best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life', 0.9999260902404785),\n",
       " ('dont', 0.9999098181724548),\n",
       " ('time', 0.9999085068702698),\n",
       " ('make', 0.9999074935913086),\n",
       " ('say', 0.9999046325683594),\n",
       " ('one', 0.9999046325683594),\n",
       " ('day', 0.999902069568634),\n",
       " ('well', 0.9999009370803833),\n",
       " ('get', 0.9998995661735535),\n",
       " ('heart', 0.9998987913131714)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_model.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('get', 0.9999110102653503),\n",
       " ('see', 0.999899685382843),\n",
       " ('already', 0.9998975396156311),\n",
       " ('b', 0.9998968243598938),\n",
       " ('thing', 0.9998928308486938),\n",
       " ('v', 0.9998915195465088),\n",
       " ('time', 0.9998908638954163),\n",
       " ('da', 0.9998903274536133),\n",
       " ('month', 0.9998899698257446),\n",
       " ('person', 0.9998892545700073)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_model.most_similar('start')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec requires **A LOT** of data to get a really good set of vectors.  Thankfully, as you saw above, Google (and others) have done this work for you, and you can usually just load in their vectors for your tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word2Vec for Machine Learning\n",
    "\n",
    "As you know, once we have vectors for examples, we can perform Machine Learning (both supervised and unsupervised).\n",
    "\n",
    "#### From Word Vectors to Document Vectors\n",
    "Consider the case of document classification.  From Word2Vec we have vectors for words, but our examples to classify are documents.  \n",
    "\n",
    "How do we get vectors for whole documents?\n",
    "\n",
    "The most common answer is to take an average of all the word vectors in a document.  Let's try that with our spam data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to take a document as a list of words and return the document vector\n",
    "def get_doc_vec(words, model):\n",
    "    good_words = []\n",
    "    for word in words:\n",
    "        # Words not in the original model will fair\n",
    "        try:\n",
    "            if model[word] is not None:\n",
    "                good_words.append(word)\n",
    "        except:\n",
    "            continue\n",
    "    # If no words are in the original model\n",
    "    if len(good_words) == 0:\n",
    "        return None\n",
    "    # Return the mean of the vectors for all the good words\n",
    "    return model[good_words].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use our function to generate the document vectors for our spam data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [-0.147819, 0.0350949, 0.0436575, 0.171935, -0...\n",
       "1       [-0.164407, 0.0420122, 0.0498686, 0.185454, -0...\n",
       "2       [-0.139702, 0.0308809, 0.0405548, 0.167575, -0...\n",
       "3       [-0.207017, 0.0507612, 0.0632579, 0.239134, -0...\n",
       "4       [-0.151833, 0.0361443, 0.0465693, 0.173738, -0...\n",
       "5       [-0.149594, 0.0332996, 0.0449054, 0.175422, -0...\n",
       "6       [-0.16496, 0.0400511, 0.0530456, 0.195745, -0....\n",
       "7       [-0.0989628, 0.0221459, 0.02983, 0.115438, -0....\n",
       "8       [-0.143616, 0.0357231, 0.0452015, 0.174036, -0...\n",
       "9       [-0.178312, 0.0394015, 0.0546452, 0.217981, -0...\n",
       "10      [-0.149825, 0.0352149, 0.0439142, 0.175553, -0...\n",
       "11      [-0.162345, 0.037339, 0.0484078, 0.191343, -0....\n",
       "12      [-0.185733, 0.0395844, 0.0555984, 0.227221, -0...\n",
       "13      [-0.121024, 0.026502, 0.0361365, 0.13955, -0.0...\n",
       "14      [-0.0876439, 0.0223248, 0.0282904, 0.0999922, ...\n",
       "15      [-0.139797, 0.0287392, 0.0408726, 0.163992, -0...\n",
       "16      [-0.175126, 0.0415515, 0.0576493, 0.208694, -0...\n",
       "17      [-0.134421, 0.0295896, 0.0385547, 0.153758, -0...\n",
       "18      [-0.15697, 0.0396652, 0.0489734, 0.178586, -0....\n",
       "19      [-0.124259, 0.0281188, 0.0342913, 0.145287, -0...\n",
       "20      [-0.0881075, 0.0181693, 0.0263274, 0.10185, -0...\n",
       "21      [-0.151877, 0.0340372, 0.0460361, 0.17221, -0....\n",
       "22      [-0.132624, 0.0306787, 0.040105, 0.152771, -0....\n",
       "23      [-0.170044, 0.0424272, 0.0505075, 0.196448, -0...\n",
       "24      [-0.166066, 0.0401822, 0.0475373, 0.19006, -0....\n",
       "25      [-0.115128, 0.0249424, 0.0363438, 0.132696, -0...\n",
       "26      [-0.163841, 0.0416462, 0.0490052, 0.192034, -0...\n",
       "27      [-0.136223, 0.0298611, 0.0404031, 0.154104, -0...\n",
       "28      [-0.191033, 0.0415067, 0.0562894, 0.218595, -0...\n",
       "29      [-0.185441, 0.0406287, 0.0545037, 0.217369, -0...\n",
       "                              ...                        \n",
       "5542    [-0.143549, 0.0341835, 0.0431347, 0.164294, -0...\n",
       "5543    [-0.153771, 0.0368172, 0.044735, 0.173256, -0....\n",
       "5544    [-0.184937, 0.0414995, 0.0538582, 0.210593, -0...\n",
       "5545    [-0.185672, 0.0485221, 0.0527761, 0.219121, -0...\n",
       "5546    [-0.145314, 0.0368678, 0.0484495, 0.170071, -0...\n",
       "5547    [-0.134987, 0.0280369, 0.0431853, 0.163706, -0...\n",
       "5548    [-0.166522, 0.0371324, 0.0479988, 0.193031, -0...\n",
       "5549    [-0.168276, 0.0354723, 0.0484268, 0.193659, -0...\n",
       "5550    [-0.20943, 0.0496954, 0.0621728, 0.23932, -0.1...\n",
       "5551    [-0.145639, 0.033306, 0.0437062, 0.169401, -0....\n",
       "5552    [-0.0852449, 0.021946, 0.0247857, 0.0988061, -...\n",
       "5553    [-0.19576, 0.0476071, 0.0574515, 0.225476, -0....\n",
       "5554    [-0.147373, 0.0356687, 0.0453645, 0.17313, -0....\n",
       "5555    [-0.148312, 0.036247, 0.0437164, 0.170095, -0....\n",
       "5556    [-0.217998, 0.0526215, 0.0642965, 0.247586, -0...\n",
       "5557    [-0.151868, 0.0251692, 0.0636735, 0.160664, -0...\n",
       "5558    [-0.186054, 0.0418404, 0.0616391, 0.22289, -0....\n",
       "5559    [-0.177639, 0.0292517, 0.074545, 0.186054, -0....\n",
       "5560    [-0.183065, 0.0480952, 0.0554726, 0.210377, -0...\n",
       "5561    [-0.140487, 0.0317348, 0.041578, 0.160458, -0....\n",
       "5562    [-0.13712, 0.034704, 0.0401134, 0.160064, -0.1...\n",
       "5563    [-0.170745, 0.0423499, 0.0537394, 0.197855, -0...\n",
       "5564    [-0.151579, 0.0340394, 0.0444895, 0.175333, -0...\n",
       "5565    [-0.0774605, 0.0184797, 0.0234782, 0.0898293, ...\n",
       "5566    [-0.172005, 0.0381173, 0.0519938, 0.20284, -0....\n",
       "5567    [-0.177358, 0.0425668, 0.0548433, 0.207493, -0...\n",
       "5568    [-0.188965, 0.0480999, 0.0575559, 0.217509, -0...\n",
       "5569    [-0.0494829, 0.00712503, 0.0174795, 0.0533658,...\n",
       "5570    [-0.164406, 0.0375017, 0.050251, 0.193734, -0....\n",
       "5571    [-0.144221, 0.0307008, 0.0456318, 0.1685, -0.1...\n",
       "Name: text, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a copy of the data to not disturb the original\n",
    "spam_data1 = spam_data.copy()\n",
    "spam_vecs = spam_data1.text.str.split().map(lambda x: get_doc_vec(x, spam_model))\n",
    "spam_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the documents have no good words in them.  Let's drop them from our dataset, but before we do add them back into the original DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5517, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add to dataframe\n",
    "spam_data1['vecs'] = spam_vecs\n",
    "# Drop the bad docs\n",
    "spam_data1 = spam_data1.dropna()\n",
    "spam_data1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's just convert the format that we have into a final DataFrame with 100 features and 1 label for use in our document classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Numpy array of the document vectors\n",
    "spam_np_vecs = np.zeros((len(spam_data1), 100))\n",
    "for i, vec in enumerate(spam_data1.vecs):\n",
    "    spam_np_vecs[i, :] = vec\n",
    "    \n",
    "# Combine the full dataframe with the labels\n",
    "spam_w2v_data = pd.concat([spam_data1.reset_index().label, pd.DataFrame(spam_np_vecs)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've arrived at a familiar point.  We have features and 1 label and we can use them to perform text classification.  As you already know how to do this, it is left for the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Exercises\n",
    "In these exercises, we'll finish experimenting with the spam data by using your word2vec vectors for text classification.  You will classify spam/ham using **both** your vectors and the pretrained Google vectors.  Then we'll move on to a richer dataset for you to perform the entire pipeline for Text Classification with Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Classification with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Use the spam word2vec dataframe from above to train and evaluate any classification algorithm for spam/ham.  **Hint**: Try a K-Nearest Neighbors Classifier.\n",
    "\n",
    "Use the Google vectors from the beginning of this notebook to generate a new spam word2vec dataframe for text classification.\n",
    "\n",
    "Build yet another spam/ham classification model using the word2vec vectors from Google.\n",
    "\n",
    "Compare the performance of the 2 classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained Word2Vec Spam Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96859903381642509"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training a Classifier with our own trained vectors\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data\n",
    "X_spam1 = spam_w2v_data.iloc[:, 1:]\n",
    "y_spam1 = spam_w2v_data.label\n",
    "X_train_spam1, X_test_spam1, y_train_spam1, y_test_spam1 = train_test_split(X_spam1, y_spam1, test_size=0.3)\n",
    "\n",
    "# Train a KNN or Logistic Regression classifier\n",
    "est = KNeighborsClassifier(algorithm='brute', metric='cosine')\n",
    "# est = LogisticRegression()\n",
    "est.fit(X_train_spam1, y_train_spam1)\n",
    "est.score(X_test_spam1, y_test_spam1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Vectors from Google Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a copy of the spam dataframe for the Google work\n",
    "spam_data2 = spam_data.copy()\n",
    "\n",
    "# Retrieve the document vectors based on google word vectors\n",
    "spam_google_vecs = spam_data2.text.str.split().map(lambda x: get_doc_vec(x, google_model))\n",
    "\n",
    "# Add to dataframe\n",
    "spam_data2['vecs'] = spam_google_vecs\n",
    "\n",
    "# Drop the bad docs\n",
    "spam_data2 = spam_data2.dropna()\n",
    "\n",
    "# Create a Numpy array of the document vectors\n",
    "spam_np_vecs = np.zeros((len(spam_data2), 300))\n",
    "for i, vec in enumerate(spam_data2.vecs):\n",
    "    spam_np_vecs[i, :] = vec\n",
    "    \n",
    "# Combine the full dataframe with the labels\n",
    "spam_google_data = pd.concat([spam_data2.reset_index().label, pd.DataFrame(spam_np_vecs)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Word2Vec Spam Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97661870503597126"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training a Classifier with Google's vectors\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data\n",
    "X_spam2 = spam_google_data.iloc[:, 1:]\n",
    "y_spam2 = spam_google_data.label\n",
    "X_train_spam2, X_test_spam2, y_train_spam2, y_test_spam2 = train_test_split(X_spam2, y_spam2, test_size=0.3)\n",
    "\n",
    "# Train a KNN or Logistic Regression classifier\n",
    "est = KNeighborsClassifier(algorithm='brute', metric='cosine')\n",
    "# est = LogisticRegression()\n",
    "est.fit(X_train_spam2, y_train_spam2)\n",
    "est.score(X_test_spam2, y_test_spam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that there is not much difference in performance between your vectors and Google's for this task (though Google's is better).  **However**, this is a fairly trivial problem/dataset, and we were already performing quite well on it.  So let's try something harder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Newsgroups Classification with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "We will be using a portion of a data set containing approximately 20,000 posts partitioned evenly across 20 different newsgroups. This data set is quite famous. We will be using a sample of this data set, containing 5 topics and about 3,000 posts.\n",
    "\n",
    "We will begin by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2956\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>otoh u get lucky unplugged replugged scsi adb ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes everyone else may wonder fred well would o...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>umm perhaps could explain right talk</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like alomar like differ opinion city likely po...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wow know uranus long way think far away</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  otoh u get lucky unplugged replugged scsi adb ...      0\n",
       "1  yes everyone else may wonder fred well would o...      4\n",
       "2               umm perhaps could explain right talk      4\n",
       "3  like alomar like differ opinion city likely po...      2\n",
       "4            wow know uranus long way think far away      4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from accessory_functions import preprocess_series_text, nltk_path\n",
    "\n",
    "topic_list = ['sci.space', 'comp.sys.mac.hardware', 'rec.autos',\n",
    "              'rec.sport.baseball', 'sci.med']\n",
    "\n",
    "# Retrieve the data into a DataFrame\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, data_home='../Data',\n",
    "                             categories=topic_list,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "ng_data = pd.DataFrame(dataset['data'], columns=['text'])\n",
    "ng_data['label'] = dataset['target']\n",
    "\n",
    "# Preprocess the text\n",
    "ng_data['text'] = preprocess_series_text(ng_data.text, nltk_path=nltk_path)\n",
    "\n",
    "\n",
    "print(len(ng_data))\n",
    "ng_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Train a Word2Vec model to generate word vectors from the 20 Newsgroups data.\n",
    "\n",
    "Use your Word2Vec model to generate document vectors from these word vectors.\n",
    "\n",
    "Combine these vectors with the 20 Newsgroups class labels to create a DataFrame for classification.\n",
    "\n",
    "Train a classification model for these five 20 Newsgroups classes and evaluate its performance.  **Hint**: Try a K-Nearest Neighbors Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Word2Vec on 20 Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brave', 0.9996107220649719),\n",
       " ('pitcher', 0.9995369911193848),\n",
       " ('offense', 0.999420166015625),\n",
       " ('stats', 0.9993765354156494),\n",
       " ('chance', 0.9992481470108032),\n",
       " ('sign', 0.9991667866706848),\n",
       " ('hitter', 0.9991663694381714),\n",
       " ('lopez', 0.9991459250450134),\n",
       " ('throw', 0.9991354942321777),\n",
       " ('ball', 0.9991125464439392)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Generate sentences for training word2vec\n",
    "sentences = ng_data.text.str.split()\n",
    "# Train a Word2Vec model\n",
    "ng_model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Did it work?\n",
    "ng_model.most_similar('baseball')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Vectors from Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the spam dataframe for the Google work\n",
    "ng_data1 = ng_data.copy()\n",
    "\n",
    "# Retrieve the document vectors based on google word vectors\n",
    "ng_vecs = ng_data1.text.str.split().map(lambda x: get_doc_vec(x, ng_model))\n",
    "\n",
    "# Add to dataframe\n",
    "ng_data1['vecs'] = ng_vecs\n",
    "\n",
    "# Drop the bad docs\n",
    "ng_data1 = ng_data1.dropna()\n",
    "\n",
    "# Create a Numpy array of the document vectors\n",
    "ng_np_vecs = np.zeros((len(ng_data1), 100))\n",
    "for i, vec in enumerate(ng_data1.vecs):\n",
    "    ng_np_vecs[i, :] = vec\n",
    "    \n",
    "# Combine the full dataframe with the labels\n",
    "ng_w2v_data = pd.concat([ng_data1.reset_index().label, pd.DataFrame(ng_np_vecs)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained Word2Vec 20 Newsgroups Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61032863849765262"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training a Classifier with our own trained vectors\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data\n",
    "X_ng1 = ng_w2v_data.iloc[:, 1:]\n",
    "y_ng1 = ng_w2v_data.label\n",
    "X_train_ng1, X_test_ng1, y_train_ng1, y_test_ng1 = train_test_split(X_ng1, y_ng1, test_size=0.3)\n",
    "\n",
    "# Train a KNN or Logistic Regression classifier\n",
    "est = KNeighborsClassifier(algorithm='brute', metric='cosine')\n",
    "# est = LogisticRegression()\n",
    "est.fit(X_train_ng1, y_train_ng1)\n",
    "est.score(X_test_ng1, y_test_ng1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Use the Google vectors to generate document vectors for the 20 Newsgroups data.\n",
    "\n",
    "Combine these vectors with the 20 Newsgroups class labels to create a DataFrame for classification.\n",
    "\n",
    "Train a classification model for these five 20 Newsgroups classes and evaluate its performance.  Train a classification model for these five 20 Newsgroups classes and evaluate its performance.  **Hint**: Try a K-Nearest Neighbors Model.\n",
    "\n",
    "Note the performance of the Google vectors vs your own Word2Vec training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 Newsgroups Document Vectors from Google Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a copy of the spam dataframe for the Google work\n",
    "ng_data2 = ng_data.copy()\n",
    "\n",
    "# Retrieve the document vectors based on google word vectors\n",
    "ng_google_vecs = ng_data2.text.str.split().map(lambda x: get_doc_vec(x, google_model))\n",
    "\n",
    "# Add to dataframe\n",
    "ng_data2['vecs'] = ng_google_vecs\n",
    "\n",
    "# Drop the bad docs\n",
    "ng_data2 = ng_data2.dropna()\n",
    "\n",
    "# Create a Numpy array of the document vectors\n",
    "ng_np_vecs = np.zeros((len(ng_data2), 300))\n",
    "for i, vec in enumerate(ng_data2.vecs):\n",
    "    ng_np_vecs[i, :] = vec\n",
    "    \n",
    "# Combine the full dataframe with the labels\n",
    "ng_google_data = pd.concat([ng_data2.reset_index().label, pd.DataFrame(ng_np_vecs)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Word2Vec Newsgroups Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87002341920374704"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training a Classifier with Google's vectors\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data\n",
    "X_ng2 = ng_google_data.iloc[:, 1:]\n",
    "y_ng2 = ng_google_data.label\n",
    "X_train_ng2, X_test_ng2, y_train_ng2, y_test_ng2 = train_test_split(X_ng2, y_ng2, test_size=0.3)\n",
    "\n",
    "# Train a KNN or Logistic Regression classifier\n",
    "est = KNeighborsClassifier(algorithm='brute', metric='cosine')\n",
    "# est = LogisticRegression()\n",
    "est.fit(X_train_ng2, y_train_ng2)\n",
    "est.score(X_test_ng2, y_test_ng2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WOW!**  Look how much better the Google vectors did.  This should demonstrate how valuable a good set of word vectors can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Recall TFIDF Vectorizer from the previous exercises.  Generate a DataFrame for classification using `TfidfVectorizer` and the 20 Newsgroups subset.\n",
    "\n",
    "Train a classification model on the TFIDF results using either Logistic Regression or Naive Bayes.\n",
    "\n",
    "Compare the results between your TFIDF model, trained word2vec, and Google word2vec.  Which does best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 Newsgroups Classification with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88500563697857948"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate TFIDF Vectors\n",
    "tfidf = TfidfVectorizer()\n",
    "ng_tfidf = tfidf.fit_transform(ng_data['text'])\n",
    "\n",
    "# Split the data\n",
    "X_ng_tfidf = ng_tfidf\n",
    "y_ng_tfidf = ng_data['label']\n",
    "X_ng_tfidf_train, X_ng_tfidf_test, y_ng_tfidf_train, y_ng_tfidf_test = train_test_split(X_ng_tfidf, y_ng_tfidf, test_size=0.3)\n",
    "\n",
    "# Train a Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_ng_tfidf_train, y_ng_tfidf_train)\n",
    "\n",
    "# Evaluate\n",
    "lr.score(X_ng_tfidf_test, y_ng_tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, in this case the TFIDF did great, on par or better than word2vec.  However, TFIDF will often do very well in cases where certain trigger words are highly useful in distinguishing between classes.  If you look through the different document categories that we chose, they are ***highly*** different.  In cases where the differences between classes are more subtle, you should expect a Word2Vec model to strongly outperform TFIDF.  With more time, you can try downloading all 20 categories from 20 Newsgroups and seeing how the various models perform.  The full set has classes that are much tougher to tease apart than the relatively disjoint subset that we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
