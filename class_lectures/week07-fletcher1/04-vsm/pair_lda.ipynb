{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code that generates a document:\n",
    "    * For n_words iterations:\n",
    "        Randomly select topic based on topic probabilities for that doc\n",
    "        With selected topic, randomly select word based on word probabilities for that topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0001648 , 0.05273598, 0.06448435, 0.00685215, 0.00318632,\n",
       "       0.01910815, 0.0095655 , 0.01695564, 0.05596396, 0.09974797,\n",
       "       0.00601108, 0.00217723, 0.07407779, 0.02503357, 0.02863812,\n",
       "       0.02343912, 0.06564616, 0.02484568, 0.01163379, 0.00124848,\n",
       "       0.04806407, 0.04158466, 0.05770537, 0.03806463, 0.00574891,\n",
       "       0.09112793, 0.03394238, 0.05648604, 0.02901851, 0.00674166])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.dirichlet(np.ones(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat cat cat cat cat kitten cat deep kitten cat\n",
      "puppy dog dog dog puppy dog dog puppy dog puppy\n",
      "learning deep deep deep learning image learning learning learning puppy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "docs = [[0.98, 0.01, 0.01],\n",
    "        [0.01, 0.98, 0.01],\n",
    "        [0.01, 0.01, 0.98]]\n",
    "topics = [[ 0.4,      0.4,   0.01,        0.01,    0.01,       0.01,\n",
    "            0.1,     0.04,   0.01,        0.01],\n",
    "          [0.01,     0.01,    0.4,         0.4,    0.01,       0.01,\n",
    "            0.1,     0.04,   0.01,        0.01],\n",
    "          [0.02,     0.02,   0.01,        0.01,     0.4,        0.4,\n",
    "           0.02,      0.1,   0.01,        0.01]]\n",
    "words =  ['cat', 'kitten',  'dog', 'puppy',  'deep', 'learning',\n",
    "          'fur',  'image',  'GPU', 'asparagus']\n",
    "\n",
    "\n",
    "def make_doc(topic_probs=None, n_words=40, verbose=True):\n",
    "    if topic_probs is None:\n",
    "        topic_probs = np.random.dirichlet(np.ones(len(topics)))\n",
    "    if verbose:\n",
    "        print('topic_probs:', topic_probs)\n",
    "    results = []\n",
    "    for _ in range(n_words):\n",
    "        i_topic = np.random.choice(len(topics), p=topic_probs)\n",
    "        topic = topics[i_topic]\n",
    "        word = np.random.choice(words, p=topic)\n",
    "        results.append(word)\n",
    "    return ' '.join(results)\n",
    "\n",
    "documents = []\n",
    "for i, doc in enumerate(docs):\n",
    "    documents.append(make_doc(topic_probs=doc, n_words=10, verbose=False))\n",
    "    print(documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate random topic distributions and word distributions within topics from dirichlet priors as below. This would be how we could initialize a **Latent Dirichlet Allocation** model before running training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_docs = 3\n",
    "num_topics = 3\n",
    "doc_params = []\n",
    "for i in range(num_docs):\n",
    "    doc_params.append(list(np.random.dirichlet(np.ones(num_topics))))\n",
    "doc_params = np.array(doc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = 10\n",
    "topic_params = []\n",
    "for i in range(num_topics):\n",
    "    topic_params.append(list(np.random.dirichlet(np.ones(num_words))))\n",
    "topic_params = np.array(topic_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(topic_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our model learns document-topic and topic-word distribution parameters, we can measure the likelihood of a particular document occuring given those parameters. We're essentially aggregating up from the probability of seeing each word in the document to the prob of the entire document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doc_prob(doc_words, doc_params, topic_params):\n",
    "    total_prob = 1\n",
    "    for word in doc_words:\n",
    "        word_prob = 0\n",
    "        word_index = words.index(word)\n",
    "        for i in range(len(topics)):\n",
    "            word_prob += (doc_params[i]) * (topic_params[i][word_index])\n",
    "        total_prob *= word_prob\n",
    "    return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.520390620941468e-06"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_doc_prob(documents[0].split(), docs[0], topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from there, we can aggregate from probability of each document up to the entire collection of documents. This gives us a way to measure how well our LDA model is doing / to actually fit it. We make iterative updates to the parameters (with a very complicated expectation-maximization algorithm) to gradually improve the likelihood of the document/word outcomes we observe based on the selected parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset_prob(documents, docs_params, topics_params):\n",
    "    total_doc_prob = 1\n",
    "    for i, document in enumerate(documents):\n",
    "        doc_prob = get_doc_prob(document.split(), docs_params[i], topics_params)\n",
    "        total_doc_prob *= doc_prob\n",
    "        print(doc_prob)\n",
    "    return total_doc_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.520390620941468e-06\n",
      "3.552880107928436e-08\n",
      "8.611442246976467e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.688986798789035e-17"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataset_prob(documents, docs, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7981358807381574e-10\n",
      "1.0249552632938032e-12\n",
      "1.510377909215812e-11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.783639830994419e-33"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataset_prob(documents, doc_params, topic_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23676883, 0.2719823 , 0.49124887],\n",
       "       [0.8164061 , 0.16713261, 0.01646129],\n",
       "       [0.28597635, 0.12655035, 0.5874733 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
