### Schedule

**9:00 am**: Pair processing [question](pair.md).

                Arianna, Angad
                Druce, Maddy
                Matt, Amine
                Rob
                Dan, Saif
                Brendon, Michael
                Andree, Iggy
                Laila, Emma
                Adam, John
                Krisztian, Goodwin
                Elizabeth, Vitoria

**10:00 am**: [Dimensionality Reduction with PCA and text data](PCA_with_text_ex.ipynb)

Check out this workbook for good examples and exercises:
* [Student version](PCA_resources/PCA_student.ipynb)
* [With solutions](PCA_resources/PCA_Solution.ipynb)

Additional Resources:

Check out the PCA_resources folder!

* [Exploring the Curse of Dimensionality](Curse_of_Dimensionality.ipynb)
* [PCA,SVD, et al](pca_SVD.ipynb)

**12:00 pm**: Food time

**1:00 pm**: Investigation Presentation


#### More Resources

 * [Curse of Dimensionality in Classification](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)
 * [Curse of Dimensionality (Wikipedia)](http://en.wikipedia.org/wiki/Curse_of_dimensionality)
 * [Curse of Dimensionality (Quora)](http://www.quora.com/What-is-the-curse-of-dimensionality)
 * Read [A tutorial on Principal Components Analysis](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf) for a very friendly introduction that starts from the very basics.
 * Read the [Stanford PCA Tutorial](http://ufldl.stanford.edu/wiki/index.php/PCA), which is just slightly mathier.
 * Read this [step-by-step walk-through](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html) of PCA with Python.
 * Read an excellent paper on [The Fundamental Theorem of Linear Algebra](http://home.eng.iastate.edu/~julied/classes/CE570/Notes/strangpaper.pdf) which goes through in an intuitive way just what SVD is all about.
 * Watch the [Chapter 10 lecture videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/) from *An Introduction to Statistical Learning* on PCA.
 * [PCA and dimensionality reduction 4 dummies](http://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)
 * [Step by step PCA math in Python](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)
 * Learn about [random projections](http://users.ics.aalto.fi/ella/publications/randproj_kdd.pdf) and try them [in sci-kit](http://scikit-learn.org/stable/modules/random_projection.html).
 * Learn about t-Distributed Stochastic Neighbor Embedding ([t-SNE](http://homepage.tudelft.nl/19j49/t-SNE.html)), another technique that can be great for making complex data visualizable. It helped [win](http://blog.kaggle.com/2012/11/02/t-distributed-stochastic-neighbor-embedding-wins-merck-viz-challenge/) a Kaggle visualization contest, for example.
