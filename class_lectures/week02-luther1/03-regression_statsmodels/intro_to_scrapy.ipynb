{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://scrapinghub.files.wordpress.com/2016/01/scrapylogo.png\" />\n",
    "\n",
    "# Introduction to Scrapy\n",
    "\n",
    "Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.\n",
    "\n",
    "Scrapy runs from the command line.\n",
    "\n",
    "[Scrapy Docs](https://doc.scrapy.org/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy vs BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy allows people to write small amounts of Python code to create a “spider” – an automated bot which can crawl web pages and scrape them. BeautifulSoup on the other hand is a helpful utility that allows a programmer to quickly extract valid data from web pages.\n",
    "\n",
    "- Data can often be incomplete in the wild – you'll want your code to be resilient to erros. Scrapy will make the process of working around incomplete data much easier for you.\n",
    "- Not all webpages are nice: pages won’t be found, servers will have errors or you could have internet connectivity issues half way through a large scrape. Scrapy lets you handle errors gracefully and even has inbuilt ability for resuming a scrape from the last page it encountered.\n",
    "- Scraping can cause issues for the sites you are targeting; for example, fetching too many pages at once can put a strain on the target server and take it offline. This will inevitably result in your spider getting banned for abuse – so it’s best to be a good citizen on the web. Scrapy allows you to be one by enabling you to easily throttle the rate at which you are scraping.\n",
    "- Scrapy can do multiple requests at the same time which allows scraping runs to be much faster. If you are writing a Python script from scratch that tries to do that, you will likely find that things can go wrong in a horrible million ways. Scrapy has years of use in actual large organisations that avoid this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following in the command line to install Scrapy package:\n",
    "```\n",
    "$ pip install scrapy\n",
    "```\n",
    "<br>\n",
    "\n",
    "**Linux (Ubuntu 12.04 or above)**\n",
    "```\n",
    "$ sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev\n",
    "\n",
    "$ sudo apt-get install python3 python3-dev  #if running python3 only\n",
    "\n",
    "$ pip install scrapy\n",
    "```\n",
    "<br>\n",
    "\n",
    "**Mac OS X**   (If pip install fails)  \n",
    "[installation guide](https://doc.scrapy.org/en/latest/intro/install.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the command line, navigate to the directory where you want to save your project and run:\n",
    "```\n",
    "$ scrapy startproject tutorial\n",
    "```\n",
    "```tutorial``` is the name of this project. This will create a ```tutorial``` directory with all the files and folders Scrapy needs to run. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spiders are a class that you define which Scrapy uses to scrape information from a website (or a group of websites).\n",
    "\n",
    "In the ```tutorial``` directory, you will find a directory called ```spiders```. Here is where we will save our scrapy script.\n",
    "\n",
    "The 'bones' of scrapy spider should look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = 'example'\n",
    "\n",
    "    custom_settings = {\n",
    "        \"DOWNLOAD_DELAY\": 3,\n",
    "        \"CONCURENT_REQUESTS_PER_DOMAIN\": 3,\n",
    "        \"HTTPCACHE_ENABLED\": True\n",
    "    }\n",
    "\n",
    "    start_urls = ['http://www.example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # script goes here!        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spider 'ExampleSpider' subclasses scrapy.Spider and defines some attributes and methods:\n",
    "\n",
    "- name: identifies the Spider. It must be unique within a project.\n",
    "\n",
    "- custom_settings: Here you can set custome settings for your spider.\n",
    "\n",
    "- start_url: Spider will begin to crawl from this url.\n",
    "\n",
    "- parse(): a method that will be called to handle the response downloaded for each of the start_urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**TIP: BE NICE TO WEBSITES**:  \n",
    "Always make sure that your crawler follows the rules defined in the website’s robots.txt file. This file is usually available at the root of a website (www.example.com/robots.txt) and it describes what a crawler should or shouldn’t crawl according to the Robots Exclusion Standard. Hitting a website too hard might harm it, slow it down for others, and might even get your IP banned!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selectors: XPath v CSS\n",
    "Selectors are patterns we can use to find one or more elements on a page so we can then work with the data within the element. Scrapy supports both CSS and XPath selectors. With XPath, you can extract data based on text elements’ contents, and not only on the page structure. So when you are scraping the web and you run into a hard-to-scrape website, XPath may be your best option and save lots of time. \n",
    "\n",
    "For additional help with XPath, check out [this tutorial](https://blog.scrapinghub.com/2016/10/27/an-introduction-to-xpath-with-examples/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy Shell\n",
    "The best way to test out your code! The scrapy shell is an interactive environment to test selectors on a specific website and see what they extract. This will help you debug your spiders before you run them. In the command line run:\n",
    "\n",
    "```\n",
    "$ scrapy shell 'https://www.example.com'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Your First Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Fesitval Scraper\n",
    "We are going to scrape information on music festivals listed on Music Festival Wizard. Specifically, we want to scrape information on all music festivals in the [United States](https://www.musicfestivalwizard.com/festival-guide/us-festivals/) and [Canada](https://www.musicfestivalwizard.com/festival-guide/canada-festivals/). Take a look at both the US and Canada festival lists and notice that they have a similar structure. \n",
    "\n",
    "We want our spider to crawl through both lists and follow the links to each individual festival listing where we can then scrape out the information we want.  \n",
    "\n",
    "Let's save the following script in a file called ```festival.py``` in the ```tutorial/tutorial/spiders``` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class FestivalSpider(scrapy.Spider):\n",
    "    name = 'music_festivals'\n",
    "\n",
    "    custom_settings = {\n",
    "        \"DOWNLOAD_DELAY\": 3,\n",
    "        \"CONCURRENT_REQUESTS_PER_DOMAIN\": 3,\n",
    "        \"HTTPCACHE_ENABLED\": True\n",
    "    }\n",
    "\n",
    "    start_urls = [\n",
    "        'https://www.musicfestivalwizard.com/festival-guide/us-festivals/',\n",
    "        'https://www.musicfestivalwizard.com/festival-guide/canada-festivals/'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract the links to the individual festival pages\n",
    "\n",
    "\n",
    "        # Follow pagination links and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Scrapy Shell to write and test the Spider. In the command line run:\n",
    "```\n",
    "$ scrapy shell 'https://www.musicfestivalwizard.com/festival-guide/us-festivals/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing Links  \n",
    "This first thing we want our Spider to do is grab the links for the individual festivals so we can go to each of those pages and extract some data. \n",
    "\n",
    "If we inspect the first link we see that address for the webpage we want is embeded in an < a > tag. Inspecting the next link shows a simlar tagging. Notice the < a > tags are inside of < span class = 'festivaltitle' >\n",
    "\n",
    "In the Scrapy Shell try:\n",
    "\n",
    "```\n",
    "In [1]: response.xpath('//span[@class=\"festivaltitle\"]/a/@href').extract()\n",
    "```\n",
    "<br>\n",
    "*Breakdown*\n",
    "- response: an object representing the webpage content that is downloaded by Scrapy\n",
    "- xpath: the selector we are using the select elements from the page. We include the directions to the elements we want in the parenthesis. \n",
    "- extract(): a method to extract elements. There is also an 'extract_first()' method.\n",
    "\n",
    "<br>\n",
    "Notice that the above returns all the links to the individual festival pages on the current page. We can now write a for loop to follow each link and scrape that page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def parse(self, response):\n",
    "        # Extract the links to the individual festival pages\n",
    "        festival_links = response.xpath('//span[@class=\"festivaltitle\"]/a/@href').extract()\n",
    "        festival_names = response.xpath('//span[@class=\"festivaltitle\"]/a/text()').extract()\n",
    "        \n",
    "        for i in range(len(festival_links)):\n",
    "            yield scrapy.Request(\n",
    "                url=festival_links[i],\n",
    "                callback=self.parse_festival, # defined later\n",
    "                meta={'url': festival_links[i], 'name': festival_names[i]}\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parse method looks for the individual festival links and then yeilds a new request for each of those links, calling the parse_festival method which we will define later. The meta parameter is essentially a dictionary that is copied to the parse_festival method. \n",
    "\n",
    "This only gets the links on page 1 of this list. What if we want to copy the links for all the pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next page\n",
    "After we've looped through all the links on the current page, we want to be able to go to the next page and do the same. Let's inspect the arrow to the next page and get the link. Using the Scrapy Shell try to figure out the Xpath to this link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def parse(self, response):\n",
    "        # Extract the links to the individual festival pages\n",
    "        festival_links = response.xpath('//span[@class=\"festivaltitle\"]/a/@href').extract()\n",
    "        festival_names = response.xpath('//span[@class=\"festivaltitle\"]/a/text()').extract()\n",
    "        \n",
    "        for i in range(len(festival_links)):\n",
    "            yield scrapy.Request(\n",
    "                url=festival_links[i],\n",
    "                callback=self.parse_festival, # defined later\n",
    "                meta={'url': festival_links[i], 'name': festival_names[i]}\n",
    "            )\n",
    "\n",
    "        # Follow pagination links and repeat\n",
    "        next_url = response.xpath('//a[@class=\"next page-numbers\"]/@href').extract()\n",
    "\n",
    "\n",
    "        yield scrapy.Request(\n",
    "            url=next_url,\n",
    "            callback=self.parse\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the link to the next page, all we need to go is call parse on that link and the cycle will repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets extract some data for each festival!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define parse_festival to scrape the data we want to get from each of the festival listing. We want to scrape the following:\n",
    "- Festival Name\n",
    "- Location\n",
    "- Dates\n",
    "- Ticket price\n",
    "- Festival website\n",
    "- Logo url\n",
    "- Lineup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Festival Name\n",
    "Recall that we already extracted the festival name and url!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Festival Name\n",
    "name = response.request.meta['name']\n",
    "url = response.request.meta['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**TIP: XPATH IN BROWSERS**:  \n",
    "When inspecting the HTML of a website, right click in the html code on the item you want and copy the XPath information.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location\n",
    "The XPath for the festival location is:\n",
    "//*[@id=\"festival-basics\"]\n",
    "\n",
    "Lets look in the scrapy shell and see what the following returns:\n",
    "```\n",
    "In [2]: response.xpath('//div[@id=\"festival-basics\"]/text()').extract()\n",
    "\n",
    "Out[2]:\n",
    "['\\r\\n',\n",
    " '\\r\\n',\n",
    " '\\r\\n',\n",
    " 'Okeechobee, FL',\n",
    " '\\r\\n',\n",
    " 'March 1-March 4, 2018',\n",
    " '\\r\\n',\n",
    " ' ',\n",
    " '\\r\\n',\n",
    " ' Yes',\n",
    " '\\r\\n',\n",
    " '\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n',\n",
    " \"Okeechobee Music & Arts Festival burst onto the Florida scene in 2016 and immediately become a fan favorite. Taking place in early March, Okeechobee is now the first major music festival of the US festival season and provides a glimpse of touring summertime artists. The festival's home at Sunshine Grove is a treat for campers with lush grassy fields, a sandy beach, and a majestic grove.  \\r\\n\\r\\n\",\n",
    " '\\r\\n',\n",
    " '\\r\\n\\r\\n',\n",
    " '\\r\\n\\r\\n',\n",
    " '\\r\\n\\r\\n\\r\\n',\n",
    " '\\r\\n\\r\\n']\n",
    "\n",
    "```\n",
    "\n",
    "We get a list with the location at index 3 and dates at index 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location = (\n",
    "    response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[3])\n",
    "\n",
    "dates = (\n",
    "    response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Website\n",
    "Let's look at the xpath for the festival's official website:\n",
    "//*[@id=\"festival-basics\"]/a\n",
    "\n",
    "Use the scrapy shell to get this one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logo and Lineup\n",
    "Try to do these on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put it all together in parse_festival :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_festival(self, response):\n",
    "        \n",
    "        name = response.request.meta['name']\n",
    "        \n",
    "        url = response.request.meta['url']\n",
    "\n",
    "        location = (\n",
    "            response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[3])\n",
    "\n",
    "        dates = (\n",
    "            response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[5])\n",
    "\n",
    "\n",
    "        website = (\n",
    "            response.xpath(\n",
    "                '//div[@id=\"festival-basics\"]/a/@href').extract_first()\n",
    "        )\n",
    "\n",
    "        logo = (\n",
    "            response.xpath(\n",
    "                '//div[@id=\"festival-basics\"]/img/@src').extract()[0]\n",
    "        )\n",
    "\n",
    "        lineup = (\n",
    "            response.xpath(\n",
    "                '//div[@class=\"lineupguide\"]/ul/li/text()').extract() +\n",
    "            response.xpath(\n",
    "                '//div[@class=\"lineupguide\"]/ul/li/a/text()').extract()\n",
    "        )\n",
    "\n",
    "        yield {\n",
    "            'url': url,\n",
    "            'name': name,\n",
    "            'location': location,\n",
    "            'dates': dates,\n",
    "            'website': website,\n",
    "            'logo': logo,\n",
    "            'lineup': lineup\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together our ```festival.py``` script should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class FestivalSpider(scrapy.Spider):\n",
    "\n",
    "    name = 'music_festivals'\n",
    "\n",
    "    custom_settings = {\n",
    "        \"DOWNLOAD_DELAY\": 3,\n",
    "        \"CONCURRENT_REQUESTS_PER_DOMAIN\": 3,\n",
    "        \"HTTPCACHE_ENABLED\": True\n",
    "    }\n",
    "\n",
    "    start_urls = [\n",
    "        'https://www.musicfestivalwizard.com/festival-guide/us-festivals/',\n",
    "        'https://www.musicfestivalwizard.com/festival-guide/canada-festivals/'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract the links to the individual festival pages\n",
    "        festival_links = response.xpath('//span[@class=\"festivaltitle\"]/a/@href').extract()\n",
    "        festival_names = response.xpath('//span[@class=\"festivaltitle\"]/a/text()').extract()\n",
    "        \n",
    "        for i in range(len(festival_links)):\n",
    "            yield scrapy.Request(\n",
    "                url=festival_links[i],\n",
    "                callback=self.parse_festival,\n",
    "                meta={'url': festival_links[i], 'name': festival_names[i]}\n",
    "            )\n",
    "\n",
    "        # Follow pagination links and repeat\n",
    "        next_url = response.xpath('//a[@class=\"next page-numbers\"]/@href').extract()\n",
    "\n",
    "\n",
    "        yield scrapy.Request(\n",
    "            url=next_url,\n",
    "            callback=self.parse\n",
    "        )\n",
    "\n",
    "    def parse_festival(self, response):\n",
    "        \n",
    "        name = response.request.meta['name']\n",
    "        \n",
    "        url = response.request.meta['url']\n",
    "\n",
    "        location = (\n",
    "            response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[3])\n",
    "\n",
    "        dates = (\n",
    "            response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[5])\n",
    "\n",
    "\n",
    "        website = (\n",
    "            response.xpath(\n",
    "                '//div[@id=\"festival-basics\"]/a/@href').extract_first()\n",
    "        )\n",
    "\n",
    "        logo = (\n",
    "            response.xpath(\n",
    "                '//div[@id=\"festival-basics\"]/img/@src').extract()[0]\n",
    "        )\n",
    "\n",
    "        lineup = (\n",
    "            response.xpath(\n",
    "                '//div[@class=\"lineupguide\"]/ul/li/text()').extract() +\n",
    "            response.xpath(\n",
    "                '//div[@class=\"lineupguide\"]/ul/li/a/text()').extract()\n",
    "        )\n",
    "\n",
    "        yield {\n",
    "            'url': url,\n",
    "            'name': name,\n",
    "            'location': location,\n",
    "            'dates': dates,\n",
    "            'website': website,\n",
    "            'logo': logo,\n",
    "            'lineup': lineup\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Your Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run your spider, in the command line, go to the project's top level directory and run:\n",
    "\n",
    "```python\n",
    "scrapy crawl music_festivals -o festival.json\n",
    "```\n",
    "\n",
    "This will run out spider named 'music_festivals' and generate a 'festivals.json' file containing all scraped data in json. Scrapy can also save data in other formats such as a csv file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Office Mojo\n",
    "Let's try to scrape some data from Box Office Mojo. Let's collect the budget info and domestic total gross for [Guardians of the Galaxy.](http://www.boxofficemojo.com/movies/?id=marvel17a.htm) Let's use the scrapy shell to test. \n",
    "```\n",
    "scrapy shell 'http://www.boxofficemojo.com/movies/?id=marvel17a.htm'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budget\n",
    "\n",
    "When we inspect the budget info and copy the XPath, we get:  \n",
    "//*[@id=\"body\"]/table[2]/tbody/td/table[1]/tbody/td[2]/table/tbody/td/center/table/tbody/td[2]/b  \n",
    "Let's see if we can grab onto something further down the XPath. In the Scrapy Shell try:\n",
    "\n",
    "```\n",
    "In [1]: response.xpath('//td[@valign= \"top\"]/b/text()').extract()\n",
    "Out[1]: [u'Action / Adventure', u'2 hrs. 17 min.', u'PG-13', u'$200 million']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genre = response.xpath('//td[@valign= \"top\"]/b/text()').extract()[0]\n",
    "run_time = response.xpath('//td[@valign= \"top\"]/b/text()').extract()[1]\n",
    "mpaa = response.xpath('//td[@valign= \"top\"]/b/text()').extract()[2]\n",
    "budget = response.xpath('//td[@valign= \"top\"]/b/text()').extract()[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Lifetime Grosses\n",
    "When we inspect the Domestic Gross and copy the XPath, we get:  \n",
    "/html/body/div[1]/div[3]/div[2]/table[2]/tbody/tr/td/table[2]/tbody/tr[2]/td/table/tbody/tr/td[1]/table/tbody/tr/td[1]/div[1]/div[2]/table/tbody/tr[1]/td[2]/b\n",
    "\n",
    "Let's try in the Scrapy Shell:\n",
    "\n",
    "```\n",
    "In [1]: response.xpath('//td[@align=\"right\"]/b/text()').extract()\n",
    "Out[1]: ['$389,813,101', '45.1%', '$863,738,748']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domestic =  response.xpath('//td[@align=\"right\"]/b/text()').extract()[0]\n",
    "foreign =  response.xpath('//td[@align=\"right\"]/b/text()').extract()[1]\n",
    "worldwide =  response.xpath('//td[@align=\"right\"]/b/text()').extract()[2]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
