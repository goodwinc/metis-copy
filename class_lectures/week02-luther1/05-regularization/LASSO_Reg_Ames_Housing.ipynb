{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Using the Ames Housing Data\n",
    "\n",
    "Using the Ames Housing Data:\n",
    "\n",
    "Dean De Cock\n",
    "Truman State University\n",
    "Journal of Statistics Education Volume 19, Number 3(2011), www.amstat.org/publications/jse/v19n3/decock.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will build some linear regression models to predict housing prices from this data.  We will split our data into training and test sets, build various models on the training data and compare their results on the test set. We will examine metrics such as *mean squared error* and *mean absolute deviation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "#import ml_insights as mli\n",
    "\n",
    "# To Plot matplotlib figures inline on the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data, Examine and Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load in the Ames Housing Data\n",
    "datafile = \"../../week02-luther/03-regression_scrape/data/Ames_Housing_Data.tsv\"\n",
    "df=pd.read_csv(datafile, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2930 entries, 0 to 2929\n",
      "Data columns (total 82 columns):\n",
      "Order              2930 non-null int64\n",
      "PID                2930 non-null int64\n",
      "MS SubClass        2930 non-null int64\n",
      "MS Zoning          2930 non-null object\n",
      "Lot Frontage       2440 non-null float64\n",
      "Lot Area           2930 non-null int64\n",
      "Street             2930 non-null object\n",
      "Alley              198 non-null object\n",
      "Lot Shape          2930 non-null object\n",
      "Land Contour       2930 non-null object\n",
      "Utilities          2930 non-null object\n",
      "Lot Config         2930 non-null object\n",
      "Land Slope         2930 non-null object\n",
      "Neighborhood       2930 non-null object\n",
      "Condition 1        2930 non-null object\n",
      "Condition 2        2930 non-null object\n",
      "Bldg Type          2930 non-null object\n",
      "House Style        2930 non-null object\n",
      "Overall Qual       2930 non-null int64\n",
      "Overall Cond       2930 non-null int64\n",
      "Year Built         2930 non-null int64\n",
      "Year Remod/Add     2930 non-null int64\n",
      "Roof Style         2930 non-null object\n",
      "Roof Matl          2930 non-null object\n",
      "Exterior 1st       2930 non-null object\n",
      "Exterior 2nd       2930 non-null object\n",
      "Mas Vnr Type       2907 non-null object\n",
      "Mas Vnr Area       2907 non-null float64\n",
      "Exter Qual         2930 non-null object\n",
      "Exter Cond         2930 non-null object\n",
      "Foundation         2930 non-null object\n",
      "Bsmt Qual          2850 non-null object\n",
      "Bsmt Cond          2850 non-null object\n",
      "Bsmt Exposure      2847 non-null object\n",
      "BsmtFin Type 1     2850 non-null object\n",
      "BsmtFin SF 1       2929 non-null float64\n",
      "BsmtFin Type 2     2849 non-null object\n",
      "BsmtFin SF 2       2929 non-null float64\n",
      "Bsmt Unf SF        2929 non-null float64\n",
      "Total Bsmt SF      2929 non-null float64\n",
      "Heating            2930 non-null object\n",
      "Heating QC         2930 non-null object\n",
      "Central Air        2930 non-null object\n",
      "Electrical         2929 non-null object\n",
      "1st Flr SF         2930 non-null int64\n",
      "2nd Flr SF         2930 non-null int64\n",
      "Low Qual Fin SF    2930 non-null int64\n",
      "Gr Liv Area        2930 non-null int64\n",
      "Bsmt Full Bath     2928 non-null float64\n",
      "Bsmt Half Bath     2928 non-null float64\n",
      "Full Bath          2930 non-null int64\n",
      "Half Bath          2930 non-null int64\n",
      "Bedroom AbvGr      2930 non-null int64\n",
      "Kitchen AbvGr      2930 non-null int64\n",
      "Kitchen Qual       2930 non-null object\n",
      "TotRms AbvGrd      2930 non-null int64\n",
      "Functional         2930 non-null object\n",
      "Fireplaces         2930 non-null int64\n",
      "Fireplace Qu       1508 non-null object\n",
      "Garage Type        2773 non-null object\n",
      "Garage Yr Blt      2771 non-null float64\n",
      "Garage Finish      2771 non-null object\n",
      "Garage Cars        2929 non-null float64\n",
      "Garage Area        2929 non-null float64\n",
      "Garage Qual        2771 non-null object\n",
      "Garage Cond        2771 non-null object\n",
      "Paved Drive        2930 non-null object\n",
      "Wood Deck SF       2930 non-null int64\n",
      "Open Porch SF      2930 non-null int64\n",
      "Enclosed Porch     2930 non-null int64\n",
      "3Ssn Porch         2930 non-null int64\n",
      "Screen Porch       2930 non-null int64\n",
      "Pool Area          2930 non-null int64\n",
      "Pool QC            13 non-null object\n",
      "Fence              572 non-null object\n",
      "Misc Feature       106 non-null object\n",
      "Misc Val           2930 non-null int64\n",
      "Mo Sold            2930 non-null int64\n",
      "Yr Sold            2930 non-null int64\n",
      "Sale Type          2930 non-null object\n",
      "Sale Condition     2930 non-null object\n",
      "SalePrice          2930 non-null int64\n",
      "dtypes: float64(11), int64(28), object(43)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "## Examine the columns, look at missing data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2925, 82)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is recommended by the data set author to remove a few outliers\n",
    "\n",
    "df = df.loc[df['Gr Liv Area']<=4000,:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are a *lot* of variables, many of which have a lot of missing values.  Let's pick out just a few columns and start building models using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smaller_df= df.loc[:,['Lot Area','Overall Qual',\n",
    "       'Overall Cond', 'Year Built', 'Year Remod/Add',\n",
    "        'Gr Liv Area', \n",
    "        'Full Bath', 'Bedroom AbvGr',\n",
    "        'Fireplaces', 'Garage Cars','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lot Area</th>\n",
       "      <th>Overall Qual</th>\n",
       "      <th>Overall Cond</th>\n",
       "      <th>Year Built</th>\n",
       "      <th>Year Remod/Add</th>\n",
       "      <th>Gr Liv Area</th>\n",
       "      <th>Full Bath</th>\n",
       "      <th>Bedroom AbvGr</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>Garage Cars</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2925.000000</td>\n",
       "      <td>2925.000000</td>\n",
       "      <td>2925.000000</td>\n",
       "      <td>2925.000000</td>\n",
       "      <td>2925.000000</td>\n",
       "      <td>2925.000000</td>\n",
       "      <td>2925.000000</td>\n",
       "      <td>2925.000000</td>\n",
       "      <td>2925.000000</td>\n",
       "      <td>2924.000000</td>\n",
       "      <td>2925.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10103.583590</td>\n",
       "      <td>6.088205</td>\n",
       "      <td>5.563761</td>\n",
       "      <td>1971.302906</td>\n",
       "      <td>1984.234188</td>\n",
       "      <td>1493.978803</td>\n",
       "      <td>1.564786</td>\n",
       "      <td>2.853675</td>\n",
       "      <td>0.596923</td>\n",
       "      <td>1.765048</td>\n",
       "      <td>180411.574701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7781.999124</td>\n",
       "      <td>1.402953</td>\n",
       "      <td>1.112262</td>\n",
       "      <td>30.242474</td>\n",
       "      <td>20.861774</td>\n",
       "      <td>486.273646</td>\n",
       "      <td>0.551386</td>\n",
       "      <td>0.827737</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.759834</td>\n",
       "      <td>78554.857286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12789.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7438.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>1126.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>129500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9428.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1993.000000</td>\n",
       "      <td>1441.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11515.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2001.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>1740.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>213500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>3820.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>625000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Lot Area  Overall Qual  Overall Cond   Year Built  Year Remod/Add  \\\n",
       "count    2925.000000   2925.000000   2925.000000  2925.000000     2925.000000   \n",
       "mean    10103.583590      6.088205      5.563761  1971.302906     1984.234188   \n",
       "std      7781.999124      1.402953      1.112262    30.242474       20.861774   \n",
       "min      1300.000000      1.000000      1.000000  1872.000000     1950.000000   \n",
       "25%      7438.000000      5.000000      5.000000  1954.000000     1965.000000   \n",
       "50%      9428.000000      6.000000      5.000000  1973.000000     1993.000000   \n",
       "75%     11515.000000      7.000000      6.000000  2001.000000     2004.000000   \n",
       "max    215245.000000     10.000000      9.000000  2010.000000     2010.000000   \n",
       "\n",
       "       Gr Liv Area    Full Bath  Bedroom AbvGr   Fireplaces  Garage Cars  \\\n",
       "count  2925.000000  2925.000000    2925.000000  2925.000000  2924.000000   \n",
       "mean   1493.978803     1.564786       2.853675     0.596923     1.765048   \n",
       "std     486.273646     0.551386       0.827737     0.645349     0.759834   \n",
       "min     334.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%    1126.000000     1.000000       2.000000     0.000000     1.000000   \n",
       "50%    1441.000000     2.000000       3.000000     1.000000     2.000000   \n",
       "75%    1740.000000     2.000000       3.000000     1.000000     2.000000   \n",
       "max    3820.000000     4.000000       8.000000     4.000000     5.000000   \n",
       "\n",
       "           SalePrice  \n",
       "count    2925.000000  \n",
       "mean   180411.574701  \n",
       "std     78554.857286  \n",
       "min     12789.000000  \n",
       "25%    129500.000000  \n",
       "50%    160000.000000  \n",
       "75%    213500.000000  \n",
       "max    625000.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smaller_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2925 entries, 0 to 2929\n",
      "Data columns (total 11 columns):\n",
      "Lot Area          2925 non-null int64\n",
      "Overall Qual      2925 non-null int64\n",
      "Overall Cond      2925 non-null int64\n",
      "Year Built        2925 non-null int64\n",
      "Year Remod/Add    2925 non-null int64\n",
      "Gr Liv Area       2925 non-null int64\n",
      "Full Bath         2925 non-null int64\n",
      "Bedroom AbvGr     2925 non-null int64\n",
      "Fireplaces        2925 non-null int64\n",
      "Garage Cars       2924 non-null float64\n",
      "SalePrice         2925 non-null int64\n",
      "dtypes: float64(1), int64(10)\n",
      "memory usage: 274.2 KB\n"
     ]
    }
   ],
   "source": [
    "smaller_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There appears to be one NA in Garage Cars - fill with 0\n",
    "smaller_df = smaller_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2925 entries, 0 to 2929\n",
      "Data columns (total 11 columns):\n",
      "Lot Area          2925 non-null int64\n",
      "Overall Qual      2925 non-null int64\n",
      "Overall Cond      2925 non-null int64\n",
      "Year Built        2925 non-null int64\n",
      "Year Remod/Add    2925 non-null int64\n",
      "Gr Liv Area       2925 non-null int64\n",
      "Full Bath         2925 non-null int64\n",
      "Bedroom AbvGr     2925 non-null int64\n",
      "Fireplaces        2925 non-null int64\n",
      "Garage Cars       2925 non-null float64\n",
      "SalePrice         2925 non-null int64\n",
      "dtypes: float64(1), int64(10)\n",
      "memory usage: 274.2 KB\n"
     ]
    }
   ],
   "source": [
    "smaller_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(smaller_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Exploration Questions\n",
    "1. Which variables seem to have strong relationships with Sales Price?\n",
    "1. The scatterplots of Year Built vs Year Add/Remod have an interesting structure.  Can you explain what is going on there?\n",
    "1. In the plot of \"Lot Area\" vs. \"SalePrice\", some outliers are making the plot less visually useful.  How can we make the plot look better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(smaller_df.loc[smaller_df['Lot Area']<50000,'Lot Area'], smaller_df.loc[smaller_df['Lot Area']<50000,'SalePrice'], alpha=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separate our features from our target\n",
    "\n",
    "X=smaller_df.loc[:,['Lot Area','Overall Qual',\n",
    "       'Overall Cond', 'Year Built', 'Year Remod/Add',\n",
    "        'Gr Liv Area', \n",
    "        'Full Bath', 'Bedroom AbvGr',\n",
    "        'Fireplaces', 'Garage Cars']]\n",
    "\n",
    "y=smaller_df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split the data 70-30 train/test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Variable Linear Regression\n",
    "To begin, we will do a simple one variable linear regression, predicting the Sales Price using the Square Footage (Gr Liv Area) of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First let us fit only on Living Area (sqft)\n",
    "selected_columns_1 = ['Gr Liv Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model1 = LinearRegression()\n",
    "lr_model1.fit(X_train.loc[:,selected_columns_1],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model1.coef_, lr_model1.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension Question\n",
    "What do the coefficients above represent?  How can they be interpreted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train['Gr Liv Area'],y_train,alpha=.05)\n",
    "vec1 = np.linspace(0,4000,1000)\n",
    "plt.plot(vec1, lr_model1.intercept_ + lr_model1.coef_[0]*vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get the predictions on the training set\n",
    "train_set_pred1 = lr_model1.predict(X_train.loc[:,selected_columns_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get the predictions on the test set\n",
    "test_set_pred1 = lr_model1.predict(X_test.loc[:,selected_columns_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the regression line on top of the data\n",
    "\n",
    "plt.scatter(X_test['Gr Liv Area'],y_test,alpha=.1)\n",
    "vec1 = np.linspace(0,4000,1000)\n",
    "plt.plot(vec1, lr_model1.intercept_ + lr_model1.coef_[0]*vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot predicted vs actual \n",
    "\n",
    "plt.scatter(test_set_pred1,y_test,alpha=.1)\n",
    "plt.plot(np.linspace(0,600000,1000),np.linspace(0,600000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Residual Plot\n",
    "## Plot predicted vs actual \n",
    "\n",
    "plt.scatter(y_test,y_test-test_set_pred1,alpha=.1)\n",
    "plt.plot(np.linspace(0,600000,1000),np.linspace(0,0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How good is our model on the test set?\n",
    "\n",
    "# Root Mean Square Error\n",
    "np.sqrt(np.mean((test_set_pred1 - y_test)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Deviation\n",
    "(np.mean(np.abs(test_set_pred1 - y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_test, test_set_pred1)), mean_absolute_error(y_test,test_set_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "We will now do a regression on several variables.  We will no longer be able to see the regression line so simply in a graph, but we can still look at the predicted vs actual and residual plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_columns_2 = ['Lot Area', 'Overall Qual', 'Year Built', 'Gr Liv Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model2 = LinearRegression()\n",
    "lr_model2.fit(X_train.loc[:,selected_columns_2],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model2.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(selected_columns_2,lr_model2.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension Question\n",
    "What do these coefficients mean?  Why does `'Gr Liv Area'` have a different coefficient this time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pred2 = lr_model2.predict(X_test.loc[:,selected_columns_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_set_pred2,y_test,alpha=.1)\n",
    "plt.plot(np.linspace(0,600000,1000),np.linspace(0,600000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_test-test_set_pred2,alpha=.1)\n",
    "plt.plot(np.linspace(0,600000,1000),np.linspace(0,0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "np.sqrt(np.mean((test_set_pred2 - y_test)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAD\n",
    "(np.mean(np.abs(test_set_pred2 - y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us try using all of the variables (in the reduced selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model3 = LinearRegression()\n",
    "lr_model3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_train.columns,lr_model3.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pred3 = lr_model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_set_pred3,y_test,alpha=.1)\n",
    "plt.plot(np.linspace(0,600000,1000),np.linspace(0,600000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,y_test-test_set_pred3,alpha=.1)\n",
    "plt.plot(np.linspace(0,600000,1000),np.linspace(0,0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "np.sqrt(np.mean((test_set_pred3 - y_test)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAD\n",
    "(np.mean(np.abs(test_set_pred3 - y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a quadratic factor\n",
    "Again, we see that our residual plot indicates some non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "\n",
    "ax[0].scatter(X_train['Overall Qual'],y_train, alpha = .05)\n",
    "\n",
    "ax[1].scatter(X_train['Gr Liv Area'],y_train, alpha = .05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try adding in `'Overall Qual'` *squared* as a predictor variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['OQ2'] = X['Overall Qual']**2\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_model4 = LinearRegression()\n",
    "lr_model4.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_train.columns,lr_model4.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pred4 = lr_model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_set_pred4,y_test,alpha=.1)\n",
    "plt.plot(np.linspace(0,600000,1000),np.linspace(0,600000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,y_test-test_set_pred4,alpha=.1)\n",
    "plt.plot(np.linspace(0,600000,1000),np.linspace(0,0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "np.sqrt(np.mean((test_set_pred4 - y_test)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAD\n",
    "(np.mean(np.abs(test_set_pred4 - y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have heard of the term *R-squared*.  R-squared measures the \"percentage of variance explained by the predictors\".  Another way to express this is to calculate the \"sum of squares\" in two different ways:\n",
    "1. Using the squared differences between our prediction and the actual value\n",
    "2. Using the squared differences between the mean of y and the actual value\n",
    "\n",
    "(So in the second variant above, you can think of it as a really naive prediction where you just guess the mean of y for every entry, regardless of the value of the predictors)\n",
    "\n",
    "The first number is called the SSE (sum of squares error).  The second number is called the SST (sum of squares total).  So then SSE/SST is what percentage of the total variance *remains* after the predictors are factored in.  Therefore $1-\\frac{SSE}{SST}$ is the *explained variance*.  $R^2 = 1-\\frac{SSE}{SST}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## R-squared\n",
    "SSE = np.sum(np.mean((test_set_pred4 - y_test)**2))\n",
    "SST = np.sum(np.mean((np.mean(y_test) - y_test)**2))\n",
    "r_squared = 1-SSE/SST\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## can also use the sklearn function\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, test_set_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each model in sklearn has a designated \"score\".\n",
    "## For LinearRegression, r^2 is the designated \"score\"\n",
    "## This function does the predictions on X_test, and then computes the R^2 in one step\n",
    "lr_model4.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "We have been playing around with adding in variables (or transformations of variables), and then seeing if they improve the model or not.  However, this can be a tedious process.\n",
    "\n",
    "Regularized Linear Regression (sometimes called Penalized Linear Regression) tries to circumvent this by changing the *cost function*.  In \"vanilla\" linear regression, the coefficients are chosen purely to minimize the Sum of Squared Errors.  In Regularized Linear Regression, there is an additional component of the cost function that penalizes the \"size\" of the coefficients.  \n",
    "\n",
    "Why penalize a coefficient?  At the simplest level, it forces a variable to be \"worth it\" in order to have a coefficient greater than zero.  This intuition extends to the size of the coefficient - in some ways it is a \"simpler model\" to have smaller coefficients (in absolute value) than larger ones.\n",
    "\n",
    "Regularized Linear Regression introduces a \"nuisance parameter\" that says how strongly we want to penalize the coefficients.  At one extreme there is no penalty, and we revert back to \"vanilla\" Linear Regression.  At the other extreme, the penalty is so onerous that we set all of the coefficients to zero.  In between these two extremes are continuous set of models.  We will discuss how to choose the best value later on when we discuss *cross-validation*.\n",
    "\n",
    "There are two main \"flavors\" of Regularized Linear Regression.  In the LASSO, we penalize the sum of the absolute values of the coefficients and in Ridge Regression we penalize the sum of the squares of the coefficients.  LASSO is often preferred for some detailed reasons we will discuss later.\n",
    "\n",
    "Let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge, LassoCV, RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_columns_3 = ['Lot Area', 'Overall Qual', 'Overall Cond', 'Year Built','Year Remod/Add', 'Gr Liv Area', 'Full Bath', 'Bedroom AbvGr',\n",
    "       'Fireplaces', 'Garage Cars', 'OQ2']\n",
    "#selected_columns_3 = ['Overall Qual','OQ2','Fireplaces','Full Bath','Fireplaces']\n",
    "\n",
    "#selected_columns_3 = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_model5 = Lasso(alpha = 1000000)\n",
    "lr_model5.fit(X_train.loc[:,selected_columns_3],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(selected_columns_3,lr_model5.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how LASSO sets many variables to 0 (with a high enough alpha parameter). This is its ** feature selection ** property. To help remember that LASSO does this and not ridge, look no further than the name: **Least Absolute Shrinkage and Selection Operator** -- the shrinkage part means making the coefficients smaller by penalizing their size in the cost function, and the selection part is zeroing out coefficients. Remember the double SS! \n",
    "\n",
    "To highlight a notable difference between LASSO and ridge, we're going to add an additional column that's Lot Area + noise, so that we have two **highly collinear columns**. Then we'll fit a ridge and LASSO model and compare their coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "\n",
    "X_train_collinear = X_train.loc[:,selected_columns_3]\n",
    "X_train_collinear['Lot Area Clone'] = (X_train_collinear['Lot Area'] + \n",
    "                                      2500 * np.random.randn(X_train.shape[0]))\n",
    "\n",
    "X_train_collinear.corr() #notice .95 correlation b/w Lot Area and its \"clone\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick aside, let's understand what happens with p-values when there is a lot of collinearity! We are much less sure about our relationships being meaningful. In this case the model does detect the right variable as having a significant relationship, but this need not be the case in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "model = sm.OLS(y_train,sm.add_constant(X_train_collinear))\n",
    "results = model.fit()\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare and contrast Ridge vs. Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_ridge = Ridge(alpha = 1000000000000)\n",
    "lr_model_ridge.fit(X_train_collinear,y_train)\n",
    "\n",
    "list(zip(X_train_collinear.columns,lr_model_ridge.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge** smoothed out all of the coefficients, **bringing them closer to 0 but not discarding any of them**. Also, it gave **roughly equal weight to the two highly collinear features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_lasso = Lasso(alpha = 100000)\n",
    "lr_model_lasso.fit(X_train_collinear,y_train)\n",
    "\n",
    "list(zip(X_train_collinear.columns,lr_model_lasso.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, **Lasso zeroed out most of the coefficients**, and **dropped the noisy collinear clone**, performing feature selection to keep the feature we really wanted. \n",
    "\n",
    "What are the Pros/Cons of either behavior?\n",
    "\n",
    "LASSO:\n",
    "* Pro: great for trimming features and focusing interpretation on a few key ones\n",
    "* Con: risk of discarding features that are actually useful\n",
    "\n",
    "Ridge:\n",
    "* Pro: great for smoothly handling multicollinearity, very nice when working with sparse features \n",
    "* Con: will never fully discard features\n",
    "\n",
    "As always, you have to validate to choose between the two. If the mapping from features to target truly depends on only a few key features, LASSO should outperform. If instead the target actually depends on many features (even if only a little dependent), Ridge should work better.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Back to the original LASSO model: diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pred5 = lr_model5.predict(X_test.loc[:,selected_columns_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_set_pred5,y_test,alpha=.1)\n",
    "plt.plot(np.linspace(0,600000,1000),np.linspace(0,600000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "np.sqrt(np.mean((test_set_pred5 - y_test)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Parameters\n",
    "One issue with Regularized Linear Regression is that the \"size\" of a coefficient may be more reflective of the units or scale of the associated variable.  For example, if a distance is measured in millimeters it will have a smaller coefficient than if it is measured in miles.  For this reason, best practice is to \"standardize\" the variables prior to running a regularized regression.  Standardizing means adding a constant and then dividing by another constant so that the resulting variable has mean 0 and standard deviation 1.  This ensures that the variables are penalized \"fairly\" with respect to one another.\n",
    "\n",
    "We demonstrate how to do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This step fits the Standard Scaler to the training data\n",
    "## Essentially it finds the mean and standard deviation of each variable in the training set\n",
    "\n",
    "std = StandardScaler()\n",
    "#std.fit(X_train.values.astype(float))\n",
    "std.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This step applies the scaler to the train set.\n",
    "## It subtracts the mean it learned in the previous step and then divides by the standard deviation\n",
    "\n",
    "X_tr = std.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Apply the scaler to the test set\n",
    "\n",
    "X_te = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that even though we put a Pandas Dataframe into the scalar, what comes out is a numpy array\n",
    "## In general, sklearn works on numpy.  It will accept pandas objects by trying to coerce them to numpy arrays\n",
    "## But it will not output any pandas objects\n",
    "\n",
    "type(X_train),type(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we can plot histograms of the transformed variables\n",
    "## Note that they seem to have means of 0 and stddevs of 1\n",
    "## (though they do not necessarily seem to be normally distributed)\n",
    "\n",
    "plt.hist(X_tr[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have appropriately scaled our variables, we can apply the LASSO as before.\n",
    "\n",
    "What we did before was technically not good practice since the variables were on different scales.  Certain variables would be (unfairly) penalized more than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit a LASSO model on the standardized data\n",
    "\n",
    "lr_model7 = Lasso(alpha = 10000)\n",
    "lr_model7.fit(X_tr,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note, it is now difficult to interpret the coefficients\n",
    "## Would have to do the math to translate back to the original scaling\n",
    "\n",
    "list(zip(X_train.columns,lr_model7.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finding the \"best\" value of lambda (alpha) with a single train/test split\n",
    "Here we will first decide on a vector of \"candidate\" alpha (lambda) values.  Then, for each candidate value, we run the following steps:\n",
    "1. Standardize the training data\n",
    "2. Fit a LASSO model on the training data\n",
    "3. Using the newly trained model, make predictions on both the training data and the test data\n",
    "4. Find the sum of squares error on both the training set and test set\n",
    "\n",
    "Then we plot how the errors change for the different values of alpha.\n",
    "\n",
    "We can then choose the alpha which gives us the best results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphalist = 10**(np.linspace(-3,4,200))\n",
    "err_vec_test = np.zeros(len(alphalist))\n",
    "err_vec_train = np.zeros(len(alphalist))\n",
    "\n",
    "for i,curr_alpha in enumerate(alphalist):\n",
    "\n",
    "    steps = [('standardize', StandardScaler()), ('lasso', Lasso(alpha = curr_alpha))]\n",
    "#    steps = [('standardize', StandardScaler()), ('ridge', Ridge(alpha = curr_alpha))]\n",
    "\n",
    "    pipe = Pipeline(steps)\n",
    "    pipe.fit(X_train.loc[:,selected_columns_3], y_train)\n",
    "    test_set_pred7 = pipe.predict(X_test.loc[:,selected_columns_3])\n",
    "    err_vec_test[i] = np.sqrt(np.mean((test_set_pred7 - y_test)**2))\n",
    "\n",
    "    train_set_pred7 = pipe.predict(X_train.loc[:,selected_columns_3])\n",
    "    err_vec_train[i] = np.sqrt(np.mean((train_set_pred7 - y_train)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the curves of both the training error and test error as alpha changes\n",
    "\n",
    "plt.plot(np.log10(alphalist),err_vec_test)\n",
    "plt.plot(np.log10(alphalist),err_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the minimum error achieved on the test set across the different alpha values we tried\n",
    "\n",
    "np.min(err_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the value of alpha that gave us the lowest error\n",
    "alphalist[np.argmin(err_vec_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LassoCV to find the best alpha via Cross-Validation\n",
    "In the previous, we found the best alpha value by comparing the performance on a single train/test split.  An even better, though more computationally intensive method, is to do a full cross-validation when comparing the different alphas.  Fortunately, the `LassoCV` in sklearn handles this \"under the hood\".  You pass the `LassoCV` the list of alphas and the number of folds to use for Cross-Validation.  It will do the following:\n",
    "\n",
    "- For each value of alpha\n",
    "1. Do a cross-validation and score the result\n",
    "- Find the value of alpha that gave the best score\n",
    "- Fit the model on all the data using the value of alpha it just found\n",
    "\n",
    "Then you can use the `predict` method of the model just as with all of our previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale the data as before\n",
    "std = StandardScaler()\n",
    "std.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Scale the Predictors on both the train and test set\n",
    "X_tr = std.transform(X_train)\n",
    "X_te = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cross validation, find the best alpha, refit the model on all the data with that alpha\n",
    "\n",
    "alphavec = 10**np.linspace(-3,9,27)\n",
    "\n",
    "lr_model8 = LassoCV(alphas = alphavec, cv=5)\n",
    "lr_model8.fit(X_tr,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the best alpha value it found\n",
    "lr_model8.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the coefficients when it refit using that best alpha\n",
    "list(zip(X_train.columns,lr_model8.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the new model\n",
    "test_set_pred8 = lr_model8.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the RMSE on the test set using that model\n",
    "np.sqrt(np.mean((test_set_pred8 - y_test)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LARS_Path\n",
    "This is a tool used to visualize *all* of the models across the range of different alpha values.  At the far left is the value of alpha where the penalty on coefficients is *so* onerous, that it just sets all of the coefficients to zero.  At the far left is when there is no penalty, and corresponds to the values of the coefficients that you would get from a \"vanilla\" linear regression.\n",
    "\n",
    "So each vertical slice corresponds to the coefficients you would get at a particular setting of alpha.  The black dotted lines indicate where a new variable \"enters\" the model (that is, its coefficient changes from 0 to non-zero).\n",
    "\n",
    "This is a good way to see which variables are most influential and how their strengths change as you change the value of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lars_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale the variables\n",
    "std = StandardScaler()\n",
    "#std.fit(X_train.values.astype(float))\n",
    "std.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr = std.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: lars_path takes numpy matrices, not pandas dataframes\n",
    "\n",
    "print(\"Computing regularization path using the LARS ...\")\n",
    "alphas, _, coefs = lars_path(X_tr, y_train.values, method='lasso', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "xx /= xx[-1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(xx, coefs.T)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "plt.xlabel('|coef| / max|coef|')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('LASSO Path')\n",
    "plt.axis('tight')\n",
    "plt.legend(X_train.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
