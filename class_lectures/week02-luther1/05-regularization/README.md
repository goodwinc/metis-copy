### Schedule

**9:00 am**: [Pair Problem](pair.md)

		Arianna, Druce
		Matt, Angad
		Maddy
		Dan, Amine
		Brendon, Rob
		Andree, Saif
		Laila, Michael
		Adam, Iggy
		Krisztian, Emma
		Elizabeth, John
		Vitoria, Goodwin

**10:00 am**: 

* [Joe's Regularization Slides](regularization_slides.pdf)
* [Additional Regularization Slides](regularization.pdf)

**11:00 am**: Regularization in code

* [Polynomial complexity vs. error plots and simple regularization code](Regularization.ipynb)
* [Regularization applied to Ames housing data (end of notebook)](LASSO_Reg_Ames_Housing.ipynb)

**12:00 am**: Lunch

**1:00 pm**: Regularization continued, if needed

**1:30 pm**: Optional Pandas Review in the Classroom

**2:30 pm**: Work!


### Further "Reading"

 * [Regularized linear regression with scikit-learn](http://www.datarobot.com/blog/regularized-linear-regression-with-scikit-learn/): This goes over some of the theory we discussed, and shows using regularization on an actual scikit learn example. It uses Lasso instead of Ridge. The only difference between Lasso and Ridge regularization is this: Ridge adds the sum of beta squares to the cost, while Lasso adds sum of beta absolute values. Other than that functional form, the idea is pretty much the same. The interface of using LinearRegression() or Ridge(alpha) or Lasso(alpha) is also exactly the same.
 * [Ten minute video lecture on regularization](https://www.youtube.com/watch?v=fx-TqOzjDbM): Another ten minute lecture by Andrew Ng on how the cost function manipulation works in regularization. It helps build intuition.
 * [Regularization Viz](http://jonhanke.github.io/Regularization-for-Regular-People/)
 
